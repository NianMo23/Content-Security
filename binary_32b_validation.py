from datetime import datetime
from tqdm import tqdm
import json
import pandas as pd
from vllm import LLM, SamplingParams
from sklearn.metrics import classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
import openpyxl
from openpyxl import Workbook

# åˆå§‹åŒ–vLLMæ¨¡å‹ - éƒ¨åˆ†GPUå¹¶è¡Œé…ç½®
import os
import logging
import sys

# ç¦ç”¨vLLMçš„è¯¦ç»†æ—¥å¿—è¾“å‡ºå’Œè¿›åº¦æ¡
logging.getLogger("vllm").setLevel(logging.CRITICAL)
logging.getLogger("transformers").setLevel(logging.CRITICAL)
logging.getLogger("torch").setLevel(logging.CRITICAL)
os.environ["TOKENIZERS_PARALLELISM"] = "false"
os.environ["VLLM_LOGGING_LEVEL"] = "CRITICAL"
os.environ["CUDA_LAUNCH_BLOCKING"] = "0"

# ç¦ç”¨æ‰€æœ‰ä¸å¿…è¦çš„è¾“å‡º
import warnings
warnings.filterwarnings("ignore")

# é‡å®šå‘stderræ¥ç¦ç”¨vLLMå†…éƒ¨è¿›åº¦æ¡
import contextlib
import io

class SuppressOutput:
    def __init__(self):
        self._original_stderr = sys.stderr
        
    def __enter__(self):
        sys.stderr = io.StringIO()
        return self
        
    def __exit__(self, exc_type, exc_val, exc_tb):
        sys.stderr = self._original_stderr

# ==================== GPUé…ç½® ====================
# è®¾ç½®è¦ä½¿ç”¨çš„GPUæ•°é‡ï¼ˆæ ¹æ®éœ€è¦ä¿®æ”¹ï¼‰
USE_GPU_COUNT = 4  # æŒ‡å®šä½¿ç”¨çš„GPUæ•°é‡ï¼Œä¾‹å¦‚ï¼šåªä½¿ç”¨2ä¸ªGPU

# è®¾ç½®å¯è§çš„GPUè®¾å¤‡ï¼ˆå¯é€‰ï¼‰
os.environ["CUDA_VISIBLE_DEVICES"] = "4,5,6,7"  # åªä½¿ç”¨GPU 0å’Œ1

# æ£€æµ‹å¯ç”¨GPUæ•°é‡
def get_gpu_count():
    try:
        import torch
        return torch.cuda.device_count()
    except:
        return 1

available_gpu_count = get_gpu_count()
actual_gpu_count = min(USE_GPU_COUNT, available_gpu_count)

print(f"ç³»ç»Ÿå¯ç”¨GPUæ•°é‡: {available_gpu_count}")
print(f"é…ç½®ä½¿ç”¨GPUæ•°é‡: {actual_gpu_count}")

try:
    # éƒ¨åˆ†GPUå¹¶è¡Œé…ç½®
    llm = LLM(
        model="/home/users/sx_zhuzz/folder/llms-from-scratch/Qwen3",
        tensor_parallel_size=actual_gpu_count,  # ä½¿ç”¨æŒ‡å®šæ•°é‡çš„GPU
        gpu_memory_utilization=0.7,  # å¤šå¡æƒ…å†µä¸‹å¯ä»¥ä½¿ç”¨æ›´é«˜çš„å†…å­˜åˆ©ç”¨ç‡
        max_model_len=8192,  # å¢åŠ æœ€å¤§é•¿åº¦ä»¥æ”¯æŒæ›´é•¿çš„prompt
        dtype="half",  # ä½¿ç”¨åŠç²¾åº¦
        trust_remote_code=True,  # å¦‚æœæ¨¡å‹éœ€è¦è‡ªå®šä¹‰ä»£ç 
        enforce_eager=False  # å¤šå¡æ—¶é€šå¸¸å¯ä»¥å¯ç”¨CUDAå›¾
    )
    sampling_params = SamplingParams(temperature=0.1, top_p=0.95, max_tokens=32)
    print(f"æˆåŠŸä½¿ç”¨ {actual_gpu_count} ä¸ªGPUåŠ è½½32Bæ¨¡å‹ï¼")
    
except Exception as e:
    print(f"æŒ‡å®šGPUæ•°é‡é…ç½®å¤±è´¥: {e}")
    print("å°è¯•ä½¿ç”¨æ›´ä¿å®ˆçš„é…ç½®...")
    
    # å¤‡ç”¨æ–¹æ¡ˆ1ï¼šå‡å°‘GPUæ•°é‡
    try:
        backup_gpu_count = max(1, actual_gpu_count // 2)
        llm = LLM(
            model="/home/users/sx_zhuzz/folder/llms-from-scratch/Qwen3",
            tensor_parallel_size=backup_gpu_count,
            gpu_memory_utilization=0.8,
            max_model_len=4096,
            dtype="half",
            trust_remote_code=True,
            enforce_eager=True
        )
        sampling_params = SamplingParams(temperature=0.1, top_p=0.95, max_tokens=32)
        print(f"ä½¿ç”¨å¤‡ç”¨é…ç½®æˆåŠŸåŠ è½½32Bæ¨¡å‹ï¼ˆ{backup_gpu_count} GPUï¼‰ï¼")
        
    except Exception as e2:
        print(f"å¤‡ç”¨é…ç½®ä¹Ÿå¤±è´¥äº†: {e2}")
        print("å°è¯•å•å¡é…ç½®...")
        
        # å¤‡ç”¨æ–¹æ¡ˆ2ï¼šå•å¡é…ç½®
        try:
            llm = LLM(
                model="/home/users/sx_zhuzz/folder/llms-from-scratch/Qwen3",
                tensor_parallel_size=1,
                gpu_memory_utilization=0.7,
                max_model_len=4096,  # å¢åŠ æœ€å¤§é•¿åº¦
                dtype="half",
                trust_remote_code=True,
                enforce_eager=True
            )
            sampling_params = SamplingParams(temperature=0.1, top_p=0.95, max_tokens=32)
            print("ä½¿ç”¨å•å¡é…ç½®åŠ è½½32Bæ¨¡å‹æˆåŠŸï¼")
        except Exception as e3:
            print(f"æ‰€æœ‰é…ç½®éƒ½å¤±è´¥äº†: {e3}")
            print("å»ºè®®ï¼š")
            print("1. æ£€æŸ¥GPUçŠ¶æ€ï¼šnvidia-smi")
            print("2. æ¸…ç†GPUå†…å­˜ï¼špkill -f python")
            print("3. æ£€æŸ¥æ¨¡å‹è·¯å¾„æ˜¯å¦æ­£ç¡®")
            print("4. è®¾ç½®ç¯å¢ƒå˜é‡ï¼šexport CUDA_VISIBLE_DEVICES=0,1")
            exit(1)

def read_csv_data(file_path: str) -> pd.DataFrame:
    """è¯»å–CSVæ•°æ®æ–‡ä»¶ï¼Œè‡ªåŠ¨æ£€æµ‹ç¼–ç æ ¼å¼"""
    
    # å°è¯•å¤šç§ç¼–ç æ ¼å¼
    encodings = ['utf-8', 'gbk', 'gb2312', 'utf-8-sig', 'latin1']
    
    for encoding in encodings:
        try:
            print(f"å°è¯•ä½¿ç”¨ {encoding} ç¼–ç è¯»å–CSVæ–‡ä»¶...")
            df = pd.read_csv(file_path, encoding=encoding)
            print(f"æˆåŠŸä½¿ç”¨ {encoding} ç¼–ç è¯»å–CSVæ–‡ä»¶ï¼Œå…±è¯»å– {len(df)} æ¡æ•°æ®")
            print(f"CSVæ–‡ä»¶åˆ—å: {list(df.columns)}")
            return df
                
        except UnicodeDecodeError:
            print(f"{encoding} ç¼–ç å¤±è´¥ï¼Œå°è¯•ä¸‹ä¸€ç§ç¼–ç ...")
            continue
        except Exception as e:
            print(f"ä½¿ç”¨ {encoding} ç¼–ç æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            continue
    
    # å¦‚æœæ‰€æœ‰ç¼–ç éƒ½å¤±è´¥ï¼Œå°è¯•è‡ªåŠ¨æ£€æµ‹ç¼–ç 
    try:
        import chardet
        with open(file_path, 'rb') as file:
            raw_data = file.read(10000)  # è¯»å–å‰10000å­—èŠ‚ç”¨äºæ£€æµ‹
            detected = chardet.detect(raw_data)
            detected_encoding = detected['encoding']
            confidence = detected['confidence']
            
            print(f"æ£€æµ‹åˆ°æ–‡ä»¶ç¼–ç : {detected_encoding} (ç½®ä¿¡åº¦: {confidence:.2f})")
            
            if detected_encoding and confidence > 0.7:
                try:
                    df = pd.read_csv(file_path, encoding=detected_encoding)
                    print(f"æˆåŠŸä½¿ç”¨æ£€æµ‹åˆ°çš„ç¼–ç  {detected_encoding} è¯»å–CSVæ–‡ä»¶ï¼Œå…±è¯»å– {len(df)} æ¡æ•°æ®")
                    return df
                    
                except Exception as e:
                    print(f"ä½¿ç”¨æ£€æµ‹åˆ°çš„ç¼–ç  {detected_encoding} å¤±è´¥: {e}")
            
    except ImportError:
        print("chardetåº“æœªå®‰è£…ï¼Œæ— æ³•è‡ªåŠ¨æ£€æµ‹ç¼–ç ")
    except Exception as e:
        print(f"è‡ªåŠ¨ç¼–ç æ£€æµ‹å¤±è´¥: {e}")
    
    print("æ‰€æœ‰ç¼–ç å°è¯•éƒ½å¤±è´¥äº†ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶æ ¼å¼æˆ–æ‰‹åŠ¨æŒ‡å®šæ­£ç¡®çš„ç¼–ç ")
    return None

def classify_batch_32b(texts: list) -> list:
    """ä½¿ç”¨32Bæ¨¡å‹æ‰¹é‡åˆ†ç±»æ–‡æœ¬ä»¥æé«˜æ•ˆç‡"""
    prompts = []
    for text in texts:
        prompt = f"""ä½ æ˜¯ä¸€ä¸ªæ–‡æœ¬å†…å®¹åˆè§„å®¡æ ¸å‘˜ï¼Œä½ çš„ä»»åŠ¡æ˜¯æ ¹æ®å¹³å°çš„è¯„åˆ¤å°ºåº¦è§„åˆ™åˆ¤æ–­ç”¨æˆ·çš„è¾“å…¥æ˜¯å¦åˆè§„ï¼Œä»”ç»†é˜…è¯»ä¸‹é¢çš„è¯„åˆ¤å°ºåº¦è§„åˆ™ã€‚

##è¯„åˆ¤å°ºåº¦è§„åˆ™##
#æ”¿æ²»å®‰å…¨#
è¿è§„èŒƒå›´ï¼š
1. ä¸‘åŒ–ã€è¯½è°¤å…šå’Œå›½å®¶é¢†å¯¼äººï¼ŒåŒ…æ‹¬ä½¿ç”¨ä¾®è¾±æ€§ç»°å·æˆ–å½±å°„è¯­è¨€ã€‚
2. é¼“åŠ¨é¢ è¦†å›½å®¶æ”¿æƒã€ä¼ æ’­åˆ†è£‚å›½å®¶ã€æ°‘æ—ä»‡æ¨ç­‰å†…å®¹ã€‚
3. å®£æ‰¬å¢ƒå¤–åååŠ¿åŠ›ã€é‚ªæ•™ç»„ç»‡ï¼Œæˆ–ç…½åŠ¨å¯¹æŠ—æ”¿åºœè¨€è®ºã€‚
4. æ¶‰åŠæ”¿æ²»æ•æ„Ÿè¯é¢˜ã€é¢ è¦†æ€§è¨€è®ºã€‚

#è‰²æƒ…ä½ä¿—#
è¿è§„èŒƒå›´ï¼š
1. æ¶‰åŠæ€§è¡Œä¸ºæå†™ã€æ€§æš—ç¤ºã€è£¸éœ²å†…å®¹ï¼Œæˆ–æ·«ç§½ã€ä½ä¿—è¨€è¾ã€‚
2. æ¨å¹¿ã€ä¼ æ’­è‰²æƒ…å†…å®¹ã€é“¾æ¥ã€å›¾æ–‡æˆ–è§†é¢‘ã€‚
3. å«æœ‰æŒ‘é€—ã€çŒ¥äºµã€è‰²æƒ…ç­‰ä¸å½“å†…å®¹ã€‚

#è¿æ³•è¿è§„#
è¿è§„èŒƒå›´ï¼š
1. å®£ä¼ ã€å¼•å¯¼å‚ä¸èµŒåšè¡Œä¸ºï¼ŒåŒ…æ‹¬èµŒçƒã€çº¿ä¸ŠèµŒåšç­‰ã€‚
2. æä¾›åšå½©ç½‘ç«™ã€é‚€è¯·ç ã€ä¸‹æ³¨æŠ€å·§ç­‰ä¿¡æ¯ã€‚
3. é¼“åŠ±ã€æ•™å”†ä»–äººå¸æ¯’ï¼Œæˆ–æå†™å¸æ¯’è¿‡ç¨‹ã€‚
4. è´©æ¯’å¼•æµã€ä»‹ç»æ¯’å“è·å–æ¸ é“ã€‚
5. å…¶ä»–è¿æ³•çŠ¯ç½ªç›¸å…³å†…å®¹ã€‚

#æš´æ#
è¿è§„èŒƒå›´ï¼š
1. é¼“å¹æš´åŠ›æç«¯æ€æƒ³ã€ä»‡æ¨ç…½åŠ¨å†…å®¹ã€‚
2. å¨èƒå®æ–½ææ€–è¡ŒåŠ¨ã€åˆ¶é€ ææ…Œæˆ–ç…½åŠ¨ä»–äººæè¢­ã€‚
3. æ¶‰åŠæš´åŠ›ã€ææ€–ä¸»ä¹‰ã€æç«¯ä¸»ä¹‰ç­‰å±é™©å†…å®¹ã€‚

#æ­§è§†#
è¿è§„èŒƒå›´ï¼š
1. åŸºäºç§æ—ã€æ€§åˆ«ã€å®—æ•™ã€å›½ç±ç­‰çš„æ­§è§†æ€§è¨€è®ºã€‚
2. ç…½åŠ¨ä»‡æ¨ã€åè§æˆ–æ­§è§†çš„å†…å®¹ã€‚
3. ä¾®è¾±ã€è´¬ä½ç‰¹å®šç¾¤ä½“çš„è¨€è®ºã€‚

#æ­£å¸¸#
æ­£å¸¸èŒƒå›´ï¼š
1. ç”Ÿæ´»ã€å­¦ä¹ ã€å·¥ä½œã€æƒ…æ„Ÿã€æ–°é—»ã€å¨±ä¹ç­‰æ™®é€šå†…å®¹ã€‚
2. ä¸æ¶‰åŠä¸Šè¿°ä»»ä½•è¿è§„å†…å®¹çš„æ­£å½“è¡¨è¾¾ã€‚
3. å®¢è§‚ã€ä¸­æ€§çš„ä¿¡æ¯åˆ†äº«å’Œè®¨è®ºã€‚

##åˆ¤æ–­è¿‡ç¨‹##
ä¸¥æ ¼æŒ‰ç…§ä¸Šè¿°è§„åˆ™ï¼Œä¸€ä¸ªæ ‡ç­¾ä¸€ä¸ªæ ‡ç­¾åˆ¤æ–­ã€‚

##è¾“å‡ºè¦æ±‚##
åªèƒ½è¾“å‡º'æ­£å¸¸'æˆ–'æ”¿æ²»å®‰å…¨'æˆ–'è‰²æƒ…ä½ä¿—'æˆ–'è¿æ³•è¿è§„'æˆ–'æš´æ'æˆ–'æ­§è§†'è¿™6ä¸ªæ ‡ç­¾ä¸­çš„ä¸€ä¸ªï¼Œä¸è¦è¾“å‡ºä»»ä½•å…¶ä»–é¢å¤–çš„ä¿¡æ¯æˆ–è§£é‡Šã€‚

ã€{text}ã€‘

è¯·åˆ¤æ–­ä¸Šè¿°æ–‡æœ¬çš„æ ‡ç­¾ï¼š"""
        prompts.append(prompt)
    
    try:
        # æŠ‘åˆ¶vLLMçš„å†…éƒ¨è¿›åº¦æ¡è¾“å‡º
        with SuppressOutput():
            outputs = llm.generate(prompts, sampling_params)
        
        results = []
        valid_labels = ['æ­£å¸¸', 'æ”¿æ²»å®‰å…¨', 'è‰²æƒ…ä½ä¿—', 'è¿æ³•è¿è§„', 'æš´æ', 'æ­§è§†']
        
        for output in outputs:
            raw_output = output.outputs[0].text.strip()
            
            # æå–æ ‡ç­¾
            found_label = "æ­£å¸¸"  # é»˜è®¤æ ‡ç­¾
            for label in valid_labels:
                if label in raw_output:
                    found_label = label
                    break
            
            results.append(found_label)
        
        return results
        
    except Exception as e:
        print(f"32Bæ¨¡å‹æ‰¹é‡è°ƒç”¨é”™è¯¯: {e}")
        return ["æ­£å¸¸"] * len(texts)  # é»˜è®¤è¿”å›æ­£å¸¸

def analyze_disagreement(original_label, predicted_label, verification_label):
    """åˆ†æå½“åŸå§‹æ ‡ç­¾å’Œé¢„æµ‹æ ‡ç­¾ä¸ä¸€è‡´æ—¶ï¼Œ32Bæ¨¡å‹çš„åˆ¤æ–­å€¾å‘"""
    if original_label == predicted_label:
        return "ä¸€è‡´", "æ— éœ€æ¯”è¾ƒ"
    
    # åŸå§‹æ ‡ç­¾å’Œé¢„æµ‹æ ‡ç­¾ä¸ä¸€è‡´çš„æƒ…å†µ
    if verification_label == original_label:
        return "ä¸ä¸€è‡´", "æ”¯æŒåŸå§‹æ ‡ç­¾"
    elif verification_label == predicted_label:
        return "ä¸ä¸€è‡´", "æ”¯æŒé¢„æµ‹æ ‡ç­¾"
    else:
        return "ä¸ä¸€è‡´", "éƒ½ä¸æ”¯æŒ"

def main():
    # è¯»å–CSVæ•°æ®æ–‡ä»¶
    data_file = './detailed_predictions_20250709_112207.csv'
    print("è¯»å–CSVæ•°æ®æ–‡ä»¶...")
    
    try:
        df = read_csv_data(data_file)
        if df is None:
            print(f"æ— æ³•è¯»å–æ–‡ä»¶ {data_file}")
            return
    except FileNotFoundError:
        print(f"æ–‡ä»¶ {data_file} ä¸å­˜åœ¨")
        return
    
    # æ£€æŸ¥å¿…è¦çš„åˆ—æ˜¯å¦å­˜åœ¨
    required_columns = ['åŸå§‹æ–‡æœ¬']  # è‡³å°‘éœ€è¦åŸå§‹æ–‡æœ¬åˆ—
    
    # æ¨æµ‹å¯èƒ½çš„åˆ—åï¼ˆè€ƒè™‘ä¸­è‹±æ–‡å’Œä¸åŒçš„å‘½åæ–¹å¼ï¼‰
    text_column = None
    original_label_column = None
    predicted_label_column = None
    
    for col in df.columns:
        if 'åŸå§‹æ–‡æœ¬' in col or 'æ–‡æœ¬' in col or 'text' in col.lower():
            text_column = col
        elif 'åŸå§‹æ ‡ç­¾' in col or 'true' in col.lower() or 'original' in col.lower():
            original_label_column = col
        elif 'é¢„æµ‹æ ‡ç­¾' in col or 'pred' in col.lower() or 'predict' in col.lower():
            predicted_label_column = col
    
    if text_column is None:
        print(f"æœªæ‰¾åˆ°æ–‡æœ¬åˆ—ï¼Œå¯ç”¨åˆ—å: {list(df.columns)}")
        return
    
    print(f"ä½¿ç”¨æ–‡æœ¬åˆ—: {text_column}")
    print(f"åŸå§‹æ ‡ç­¾åˆ—: {original_label_column}")
    print(f"é¢„æµ‹æ ‡ç­¾åˆ—: {predicted_label_column}")
    
    total = len(df)
    print(f"å¼€å§‹å¤„ç† {total} æ¡æ•°æ®...")
    
    # æ‰¹é‡å¤„ç†ä»¥æé«˜æ•ˆç‡ - å¤šå¡ä¼˜åŒ–
    # 4å¼ å¡ï¼Œæ¯å¼ å¡å¤„ç†128æ ·æœ¬ï¼Œæ€»æ‰¹æ¬¡å¤§å°ä¸º512
    batch_size = 32 * actual_gpu_count  # æ ¹æ®GPUæ•°é‡åŠ¨æ€è°ƒæ•´
    print(f"ä½¿ç”¨æ‰¹å¤„ç†å¤§å°: {batch_size} (æ¯å¼ GPUå¤„ç†çº¦{batch_size//actual_gpu_count}ä¸ªæ ·æœ¬)")
    print(f"é¢„è®¡å•æ‰¹æ¬¡å¯å……åˆ†åˆ©ç”¨{actual_gpu_count}å¼ GPUçš„å¹¶è¡Œèƒ½åŠ›")
    
    # å­˜å‚¨ç»“æœ
    verification_labels = []
    
    # åˆ›å»ºè¿›åº¦æ¡
    progress_bar = tqdm(range(0, total, batch_size),
                       desc="32Bæ¨¡å‹æ£€éªŒè¿›åº¦",
                       unit="batch",
                       ncols=120,
                       bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [å·²å¤„ç†: {postfix}]')
    
    processed_count = 0
    import time
    start_time = time.time()
    
    for i in progress_bar:
        batch_start_time = time.time()
        batch_df = df.iloc[i:i+batch_size]
        texts = batch_df[text_column].astype(str).tolist()
        
        # ä½¿ç”¨32Bæ¨¡å‹æ‰¹é‡åˆ†ç±»
        batch_verification_labels = classify_batch_32b(texts)
        
        # ç¡®ä¿æ‰¹æ¬¡ç»“æœé•¿åº¦ä¸æ‰¹æ¬¡æ•°æ®é•¿åº¦ä¸€è‡´
        if len(batch_verification_labels) != len(batch_df):
            print(f"è­¦å‘Šï¼šæ‰¹æ¬¡ {i//batch_size + 1} ç»“æœé•¿åº¦ä¸åŒ¹é…ï¼")
            print(f"  æ‰¹æ¬¡æ•°æ®é•¿åº¦: {len(batch_df)}")
            print(f"  æ‰¹æ¬¡ç»“æœé•¿åº¦: {len(batch_verification_labels)}")
            # æˆªæ–­æˆ–å¡«å……åˆ°æ­£ç¡®é•¿åº¦
            if len(batch_verification_labels) > len(batch_df):
                batch_verification_labels = batch_verification_labels[:len(batch_df)]
            else:
                batch_verification_labels.extend(['æ­£å¸¸'] * (len(batch_df) - len(batch_verification_labels)))
        
        verification_labels.extend(batch_verification_labels)
        
        processed_count += len(batch_df)
        batch_time = time.time() - batch_start_time
        avg_time_per_sample = batch_time / len(batch_df)
        throughput = len(batch_df) / batch_time
        
        # æ›´æ–°è¿›åº¦æ¡æ˜¾ç¤ºä¿¡æ¯
        progress_bar.set_postfix({
            'æ ·æœ¬': f'{processed_count}/{total}',
            'æ‰¹æ¬¡å¤§å°': len(batch_df),
            'ååé‡': f'{throughput:.1f}æ ·æœ¬/ç§’',
            'æ‰¹æ¬¡è€—æ—¶': f'{batch_time:.1f}ç§’'
        })
    
    progress_bar.close()
    
    # æœ€ç»ˆé•¿åº¦æ£€æŸ¥å’Œä¿®æ­£
    print(f"\n=== é•¿åº¦æ£€æŸ¥ ===")
    print(f"DataFrameé•¿åº¦: {len(df)}")
    print(f"éªŒè¯æ ‡ç­¾é•¿åº¦: {len(verification_labels)}")
    
    if len(verification_labels) != len(df):
        print(f"âš ï¸ é•¿åº¦ä¸åŒ¹é…ï¼å·®å¼‚: {len(verification_labels) - len(df)}")
        
        if len(verification_labels) > len(df):
            # å¦‚æœæ ‡ç­¾è¿‡å¤šï¼Œæˆªæ–­åˆ°æ­£ç¡®é•¿åº¦
            verification_labels = verification_labels[:len(df)]
            print(f"âœ‚ï¸ å·²æˆªæ–­éªŒè¯æ ‡ç­¾åˆ° {len(verification_labels)} ä¸ª")
        else:
            # å¦‚æœæ ‡ç­¾ä¸è¶³ï¼Œç”¨é»˜è®¤å€¼å¡«å……
            missing_count = len(df) - len(verification_labels)
            verification_labels.extend(['æ­£å¸¸'] * missing_count)
            print(f"ğŸ“ å·²å¡«å…… {missing_count} ä¸ªé»˜è®¤æ ‡ç­¾")
    
    # å†æ¬¡ç¡®è®¤é•¿åº¦åŒ¹é…
    assert len(verification_labels) == len(df), f"é•¿åº¦ä»ä¸åŒ¹é…: {len(verification_labels)} vs {len(df)}"
    print("âœ… é•¿åº¦æ£€æŸ¥é€šè¿‡")
    
    # æ·»åŠ 32Bæ¨¡å‹æ£€éªŒæ ‡ç­¾åˆ—
    df['32Bæ¨¡å‹æ£€éªŒæ ‡ç­¾'] = verification_labels
    
    # åˆ†æä¸ä¸€è‡´æƒ…å†µï¼ˆå¦‚æœæœ‰åŸå§‹æ ‡ç­¾å’Œé¢„æµ‹æ ‡ç­¾åˆ—ï¼‰
    disagreement_analysis = {
        'æ”¯æŒåŸå§‹æ ‡ç­¾': 0,
        'æ”¯æŒé¢„æµ‹æ ‡ç­¾': 0, 
        'éƒ½ä¸æ”¯æŒ': 0,
        'æ€»ä¸ä¸€è‡´æ•°': 0
    }
    
    if original_label_column and predicted_label_column:
        for idx, row in df.iterrows():
            original = row[original_label_column]
            predicted = row[predicted_label_column]
            verification = row['32Bæ¨¡å‹æ£€éªŒæ ‡ç­¾']
            
            consistency, support = analyze_disagreement(original, predicted, verification)
            
            if consistency == "ä¸ä¸€è‡´":
                disagreement_analysis['æ€»ä¸ä¸€è‡´æ•°'] += 1
                if support == "æ”¯æŒåŸå§‹æ ‡ç­¾":
                    disagreement_analysis['æ”¯æŒåŸå§‹æ ‡ç­¾'] += 1
                elif support == "æ”¯æŒé¢„æµ‹æ ‡ç­¾":
                    disagreement_analysis['æ”¯æŒé¢„æµ‹æ ‡ç­¾'] += 1
                elif support == "éƒ½ä¸æ”¯æŒ":
                    disagreement_analysis['éƒ½ä¸æ”¯æŒ'] += 1
    
    # ä¿å­˜ç»“æœ
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    output_filename = f"zzz/enhanced_with_32b_verification_{timestamp}.csv"
    
    # ä¿å­˜å¢å¼ºçš„CSVæ–‡ä»¶
    df.to_csv(output_filename, index=False, encoding='utf-8-sig')
    print(f"âœ… å¢å¼ºçš„CSVæ–‡ä»¶å·²ä¿å­˜: {output_filename}")
    
    # è¾“å‡ºç»Ÿè®¡ç»“æœ
    print(f"\n=== 32Bæ¨¡å‹æ£€éªŒç»“æœç»Ÿè®¡ ===")
    print(f"æ€»æ ·æœ¬æ•°: {total}")
    print(f"32Bæ¨¡å‹æ£€éªŒå®Œæˆ")
    
    if original_label_column and predicted_label_column:
        print(f"\n=== ä¸ä¸€è‡´æƒ…å†µåˆ†æ ===")
        print(f"åŸå§‹æ ‡ç­¾ä¸é¢„æµ‹æ ‡ç­¾ä¸ä¸€è‡´çš„æ ·æœ¬æ•°: {disagreement_analysis['æ€»ä¸ä¸€è‡´æ•°']}")
        
        if disagreement_analysis['æ€»ä¸ä¸€è‡´æ•°'] > 0:
            print(f"åœ¨ä¸ä¸€è‡´çš„æƒ…å†µä¸‹ï¼š")
            print(f"  32Bæ¨¡å‹æ”¯æŒåŸå§‹æ ‡ç­¾: {disagreement_analysis['æ”¯æŒåŸå§‹æ ‡ç­¾']} ({disagreement_analysis['æ”¯æŒåŸå§‹æ ‡ç­¾']/disagreement_analysis['æ€»ä¸ä¸€è‡´æ•°']:.1%})")
            print(f"  32Bæ¨¡å‹æ”¯æŒé¢„æµ‹æ ‡ç­¾: {disagreement_analysis['æ”¯æŒé¢„æµ‹æ ‡ç­¾']} ({disagreement_analysis['æ”¯æŒé¢„æµ‹æ ‡ç­¾']/disagreement_analysis['æ€»ä¸ä¸€è‡´æ•°']:.1%})")
            print(f"  32Bæ¨¡å‹éƒ½ä¸æ”¯æŒ: {disagreement_analysis['éƒ½ä¸æ”¯æŒ']} ({disagreement_analysis['éƒ½ä¸æ”¯æŒ']/disagreement_analysis['æ€»ä¸ä¸€è‡´æ•°']:.1%})")
        
        # è®¡ç®—32Bæ¨¡å‹ä¸åŸå§‹æ ‡ç­¾ã€é¢„æµ‹æ ‡ç­¾çš„ä¸€è‡´æ€§
        if original_label_column:
            original_agreement = sum(1 for i in range(len(df)) if df.iloc[i][original_label_column] == df.iloc[i]['32Bæ¨¡å‹æ£€éªŒæ ‡ç­¾'])
            original_agreement_rate = original_agreement / total
            print(f"\n32Bæ¨¡å‹ä¸åŸå§‹æ ‡ç­¾ä¸€è‡´ç‡: {original_agreement_rate:.2%} ({original_agreement}/{total})")
        
        if predicted_label_column:
            predicted_agreement = sum(1 for i in range(len(df)) if df.iloc[i][predicted_label_column] == df.iloc[i]['32Bæ¨¡å‹æ£€éªŒæ ‡ç­¾'])
            predicted_agreement_rate = predicted_agreement / total
            print(f"32Bæ¨¡å‹ä¸é¢„æµ‹æ ‡ç­¾ä¸€è‡´ç‡: {predicted_agreement_rate:.2%} ({predicted_agreement}/{total})")
    
    # ç»Ÿè®¡å„æ ‡ç­¾åˆ†å¸ƒ
    print(f"\n=== 32Bæ¨¡å‹æ£€éªŒæ ‡ç­¾åˆ†å¸ƒ ===")
    label_counts = df['32Bæ¨¡å‹æ£€éªŒæ ‡ç­¾'].value_counts()
    for label, count in label_counts.items():
        print(f"{label}: {count} ({count/total:.1%})")
    
    # ä¿å­˜è¯¦ç»†ç»Ÿè®¡æŠ¥å‘Š
    report_data = {
        'timestamp': timestamp,
        'total_samples': total,
        'disagreement_analysis': disagreement_analysis,
        'label_distribution': label_counts.to_dict()
    }
    
    if original_label_column and predicted_label_column:
        report_data['original_agreement_rate'] = original_agreement_rate
        report_data['predicted_agreement_rate'] = predicted_agreement_rate
    
    report_filename = f"zzz/32b_verification_report_{timestamp}.json"
    with open(report_filename, 'w', encoding='utf-8') as f:
        json.dump(report_data, f, ensure_ascii=False, indent=2)
    
    print(f"\n=== æ–‡ä»¶ä¿å­˜æ€»ç»“ ===")
    print(f"ğŸ“„ å¢å¼ºçš„CSVæ–‡ä»¶: {output_filename}")
    print(f"   - åŸæœ‰åˆ— + '32Bæ¨¡å‹æ£€éªŒæ ‡ç­¾'åˆ—")
    print(f"ğŸ“‹ ç»Ÿè®¡æŠ¥å‘ŠJSON: {report_filename}")
    print(f"   - è¯¦ç»†çš„ä¸€è‡´æ€§å’Œåˆ†å¸ƒç»Ÿè®¡")

if __name__ == "__main__":
    main()