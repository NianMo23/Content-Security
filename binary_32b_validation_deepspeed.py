from datetime import datetime
from tqdm import tqdm
import json
import pandas as pd
import torch
import deepspeed
from transformers import AutoTokenizer, AutoModelForCausalLM
import matplotlib.pyplot as plt
import seaborn as sns
import openpyxl
from openpyxl import Workbook

# åˆå§‹åŒ–
import os
import logging
import sys

# ç¦ç”¨è¯¦ç»†æ—¥å¿—è¾“å‡º
logging.getLogger("transformers").setLevel(logging.CRITICAL)
logging.getLogger("torch").setLevel(logging.CRITICAL)
os.environ["TOKENIZERS_PARALLELISM"] = "false"

# ç¦ç”¨æ‰€æœ‰ä¸å¿…è¦çš„è¾“å‡º
import warnings
warnings.filterwarnings("ignore")

# ==================== GPUé…ç½® ====================
# è®¾ç½®è¦ä½¿ç”¨çš„GPUæ•°é‡ï¼ˆæ ¹æ®éœ€è¦ä¿®æ”¹ï¼‰
USE_GPU_COUNT = 4  # æŒ‡å®šä½¿ç”¨çš„GPUæ•°é‡

# è®¾ç½®å¯è§çš„GPUè®¾å¤‡
os.environ["CUDA_VISIBLE_DEVICES"] = "4,5,6,7"

# æ£€æµ‹å¯ç”¨GPUæ•°é‡
def get_gpu_count():
    try:
        return torch.cuda.device_count()
    except:
        return 1

available_gpu_count = get_gpu_count()
actual_gpu_count = min(USE_GPU_COUNT, available_gpu_count)

print(f"ç³»ç»Ÿå¯ç”¨GPUæ•°é‡: {available_gpu_count}")
print(f"é…ç½®ä½¿ç”¨GPUæ•°é‡: {actual_gpu_count}")

# DeepSpeedé…ç½®
deepspeed_config = {
    "train_batch_size": 16,
    "train_micro_batch_size_per_gpu": 4,
    "steps_per_print": 100,
    "zero_optimization": {
        "stage": 3,
        "offload_optimizer": {
            "device": "cpu",
            "pin_memory": True
        },
        "offload_param": {
            "device": "cpu",
            "pin_memory": True
        },
        "overlap_comm": True,
        "contiguous_gradients": True,
        "sub_group_size": 1e9,
        "reduce_bucket_size": "auto",
        "stage3_prefetch_bucket_size": "auto",
        "stage3_param_persistence_threshold": "auto",
        "stage3_max_live_parameters": 1e9,
        "stage3_max_reuse_distance": 1e9,
        "stage3_gather_16bit_weights_on_model_save": True
    },
    "fp16": {
        "enabled": True,
        "auto_cast": False,
        "loss_scale": 0,
        "initial_scale_power": 16,
        "loss_scale_window": 1000,
        "hysteresis": 2,
        "min_loss_scale": 1
    },
    "optimizer": {
        "type": "AdamW",
        "params": {
            "lr": 3e-5,
            "betas": [0.8, 0.999],
            "eps": 1e-8,
            "weight_decay": 3e-7
        }
    },
    "scheduler": {
        "type": "WarmupLR",
        "params": {
            "warmup_min_lr": 0,
            "warmup_max_lr": 3e-5,
            "warmup_num_steps": 1000
        }
    }
}

# å…¨å±€å˜é‡å­˜å‚¨æ¨¡å‹å’Œtokenizer
model = None
tokenizer = None
ds_engine = None

def initialize_deepspeed_model():
    """åˆå§‹åŒ–DeepSpeedæ¨¡å‹"""
    global model, tokenizer, ds_engine
    
    model_path = "/home/users/sx_zhuzz/folder/llms-from-scratch/Qwen3"
    
    try:
        print("åŠ è½½tokenizer...")
        tokenizer = AutoTokenizer.from_pretrained(
            model_path,
            trust_remote_code=True,
            padding_side='left'
        )
        
        # è®¾ç½®pad_token
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        print("åŠ è½½æ¨¡å‹...")
        model = AutoModelForCausalLM.from_pretrained(
            model_path,
            torch_dtype=torch.float16,
            trust_remote_code=True,
            device_map=None  # DeepSpeedä¼šç®¡ç†è®¾å¤‡åˆ†é…
        )
        
        print("åˆå§‹åŒ–DeepSpeed...")
        # åˆ›å»ºè™šæ‹Ÿä¼˜åŒ–å™¨å‚æ•°ï¼ˆæ¨ç†æ—¶ä¸éœ€è¦çœŸå®ä¼˜åŒ–å™¨ï¼‰
        dummy_optimizer = torch.optim.AdamW(model.parameters(), lr=3e-5)
        
        # åˆå§‹åŒ–DeepSpeedå¼•æ“
        ds_engine, _, _, _ = deepspeed.initialize(
            model=model,
            optimizer=dummy_optimizer,
            config=deepspeed_config,
            dist_init_required=True
        )
        
        print(f"æˆåŠŸä½¿ç”¨DeepSpeedåˆå§‹åŒ–32Bæ¨¡å‹ï¼")
        print(f"DeepSpeedå¼•æ“çŠ¶æ€: {ds_engine.global_rank}/{ds_engine.world_size}")
        return True
        
    except Exception as e:
        print(f"DeepSpeedåˆå§‹åŒ–å¤±è´¥: {e}")
        return False

def read_csv_data(file_path: str) -> pd.DataFrame:
    """è¯»å–CSVæ•°æ®æ–‡ä»¶ï¼Œè‡ªåŠ¨æ£€æµ‹ç¼–ç æ ¼å¼"""
    
    # å°è¯•å¤šç§ç¼–ç æ ¼å¼
    encodings = ['utf-8', 'gbk', 'gb2312', 'utf-8-sig', 'latin1']
    
    for encoding in encodings:
        try:
            print(f"å°è¯•ä½¿ç”¨ {encoding} ç¼–ç è¯»å–CSVæ–‡ä»¶...")
            df = pd.read_csv(file_path, encoding=encoding)
            print(f"æˆåŠŸä½¿ç”¨ {encoding} ç¼–ç è¯»å–CSVæ–‡ä»¶ï¼Œå…±è¯»å– {len(df)} æ¡æ•°æ®")
            print(f"CSVæ–‡ä»¶åˆ—å: {list(df.columns)}")
            return df
                
        except UnicodeDecodeError:
            print(f"{encoding} ç¼–ç å¤±è´¥ï¼Œå°è¯•ä¸‹ä¸€ç§ç¼–ç ...")
            continue
        except Exception as e:
            print(f"ä½¿ç”¨ {encoding} ç¼–ç æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            continue
    
    # å¦‚æœæ‰€æœ‰ç¼–ç éƒ½å¤±è´¥ï¼Œå°è¯•è‡ªåŠ¨æ£€æµ‹ç¼–ç 
    try:
        import chardet
        with open(file_path, 'rb') as file:
            raw_data = file.read(10000)  # è¯»å–å‰10000å­—èŠ‚ç”¨äºæ£€æµ‹
            detected = chardet.detect(raw_data)
            detected_encoding = detected['encoding']
            confidence = detected['confidence']
            
            print(f"æ£€æµ‹åˆ°æ–‡ä»¶ç¼–ç : {detected_encoding} (ç½®ä¿¡åº¦: {confidence:.2f})")
            
            if detected_encoding and confidence > 0.7:
                try:
                    df = pd.read_csv(file_path, encoding=detected_encoding)
                    print(f"æˆåŠŸä½¿ç”¨æ£€æµ‹åˆ°çš„ç¼–ç  {detected_encoding} è¯»å–CSVæ–‡ä»¶ï¼Œå…±è¯»å– {len(df)} æ¡æ•°æ®")
                    return df
                    
                except Exception as e:
                    print(f"ä½¿ç”¨æ£€æµ‹åˆ°çš„ç¼–ç  {detected_encoding} å¤±è´¥: {e}")
            
    except ImportError:
        print("chardetåº“æœªå®‰è£…ï¼Œæ— æ³•è‡ªåŠ¨æ£€æµ‹ç¼–ç ")
    except Exception as e:
        print(f"è‡ªåŠ¨ç¼–ç æ£€æµ‹å¤±è´¥: {e}")
    
    print("æ‰€æœ‰ç¼–ç å°è¯•éƒ½å¤±è´¥äº†ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶æ ¼å¼æˆ–æ‰‹åŠ¨æŒ‡å®šæ­£ç¡®çš„ç¼–ç ")
    return None

def classify_batch_deepspeed(texts: list) -> list:
    """ä½¿ç”¨DeepSpeedæ¨¡å‹æ‰¹é‡åˆ†ç±»æ–‡æœ¬"""
    global ds_engine, tokenizer
    
    prompts = []
    for text in texts:
        prompt = f"""ä½ æ˜¯ä¸€ä¸ªæ–‡æœ¬å†…å®¹åˆè§„å®¡æ ¸å‘˜ï¼Œä½ çš„ä»»åŠ¡æ˜¯æ ¹æ®å¹³å°çš„è¯„åˆ¤å°ºåº¦è§„åˆ™åˆ¤æ–­ç”¨æˆ·çš„è¾“å…¥æ˜¯å¦åˆè§„ï¼Œä»”ç»†é˜…è¯»ä¸‹é¢çš„è¯„åˆ¤å°ºåº¦è§„åˆ™ã€‚

##è¯„åˆ¤å°ºåº¦è§„åˆ™##
#æ”¿æ²»å®‰å…¨#
è¿è§„èŒƒå›´ï¼š
1. ä¸‘åŒ–ã€è¯½è°¤å…šå’Œå›½å®¶é¢†å¯¼äººï¼ŒåŒ…æ‹¬ä½¿ç”¨ä¾®è¾±æ€§ç»°å·æˆ–å½±å°„è¯­è¨€ã€‚
2. é¼“åŠ¨é¢ è¦†å›½å®¶æ”¿æƒã€ä¼ æ’­åˆ†è£‚å›½å®¶ã€æ°‘æ—ä»‡æ¨ç­‰å†…å®¹ã€‚
3. å®£æ‰¬å¢ƒå¤–åååŠ¿åŠ›ã€é‚ªæ•™ç»„ç»‡ï¼Œæˆ–ç…½åŠ¨å¯¹æŠ—æ”¿åºœè¨€è®ºã€‚
4. æ¶‰åŠæ”¿æ²»æ•æ„Ÿè¯é¢˜ã€é¢ è¦†æ€§è¨€è®ºã€‚

#è‰²æƒ…ä½ä¿—#
è¿è§„èŒƒå›´ï¼š
1. æ¶‰åŠæ€§è¡Œä¸ºæå†™ã€æ€§æš—ç¤ºã€è£¸éœ²å†…å®¹ï¼Œæˆ–æ·«ç§½ã€ä½ä¿—è¨€è¾ã€‚
2. æ¨å¹¿ã€ä¼ æ’­è‰²æƒ…å†…å®¹ã€é“¾æ¥ã€å›¾æ–‡æˆ–è§†é¢‘ã€‚
3. å«æœ‰æŒ‘é€—ã€çŒ¥äºµã€è‰²æƒ…ç­‰ä¸å½“å†…å®¹ã€‚

#è¿æ³•è¿è§„#
è¿è§„èŒƒå›´ï¼š
1. å®£ä¼ ã€å¼•å¯¼å‚ä¸èµŒåšè¡Œä¸ºï¼ŒåŒ…æ‹¬èµŒçƒã€çº¿ä¸ŠèµŒåšç­‰ã€‚
2. æä¾›åšå½©ç½‘ç«™ã€é‚€è¯·ç ã€ä¸‹æ³¨æŠ€å·§ç­‰ä¿¡æ¯ã€‚
3. é¼“åŠ±ã€æ•™å”†ä»–äººå¸æ¯’ï¼Œæˆ–æå†™å¸æ¯’è¿‡ç¨‹ã€‚
4. è´©æ¯’å¼•æµã€ä»‹ç»æ¯’å“è·å–æ¸ é“ã€‚
5. å…¶ä»–è¿æ³•çŠ¯ç½ªç›¸å…³å†…å®¹ã€‚

#æš´æ#
è¿è§„èŒƒå›´ï¼š
1. é¼“å¹æš´åŠ›æç«¯æ€æƒ³ã€ä»‡æ¨ç…½åŠ¨å†…å®¹ã€‚
2. å¨èƒå®æ–½ææ€–è¡ŒåŠ¨ã€åˆ¶é€ ææ…Œæˆ–ç…½åŠ¨ä»–äººæè¢­ã€‚
3. æ¶‰åŠæš´åŠ›ã€ææ€–ä¸»ä¹‰ã€æç«¯ä¸»ä¹‰ç­‰å±é™©å†…å®¹ã€‚

#æ­§è§†#
è¿è§„èŒƒå›´ï¼š
1. åŸºäºç§æ—ã€æ€§åˆ«ã€å®—æ•™ã€å›½ç±ç­‰çš„æ­§è§†æ€§è¨€è®ºã€‚
2. ç…½åŠ¨ä»‡æ¨ã€åè§æˆ–æ­§è§†çš„å†…å®¹ã€‚
3. ä¾®è¾±ã€è´¬ä½ç‰¹å®šç¾¤ä½“çš„è¨€è®ºã€‚

#æ­£å¸¸#
æ­£å¸¸èŒƒå›´ï¼š
1. ç”Ÿæ´»ã€å­¦ä¹ ã€å·¥ä½œã€æƒ…æ„Ÿã€æ–°é—»ã€å¨±ä¹ç­‰æ™®é€šå†…å®¹ã€‚
2. ä¸æ¶‰åŠä¸Šè¿°ä»»ä½•è¿è§„å†…å®¹çš„æ­£å½“è¡¨è¾¾ã€‚
3. å®¢è§‚ã€ä¸­æ€§çš„ä¿¡æ¯åˆ†äº«å’Œè®¨è®ºã€‚

##åˆ¤æ–­è¿‡ç¨‹##
ä¸¥æ ¼æŒ‰ç…§ä¸Šè¿°è§„åˆ™ï¼Œä¸€ä¸ªæ ‡ç­¾ä¸€ä¸ªæ ‡ç­¾åˆ¤æ–­ã€‚

##è¾“å‡ºè¦æ±‚##
åªèƒ½è¾“å‡º'æ­£å¸¸'æˆ–'æ”¿æ²»å®‰å…¨'æˆ–'è‰²æƒ…ä½ä¿—'æˆ–'è¿æ³•è¿è§„'æˆ–'æš´æ'æˆ–'æ­§è§†'è¿™6ä¸ªæ ‡ç­¾ä¸­çš„ä¸€ä¸ªï¼Œä¸è¦è¾“å‡ºä»»ä½•å…¶ä»–é¢å¤–çš„ä¿¡æ¯æˆ–è§£é‡Šã€‚

ã€{text}ã€‘

è¯·åˆ¤æ–­ä¸Šè¿°æ–‡æœ¬çš„æ ‡ç­¾ï¼š"""
        prompts.append(prompt)
    
    try:
        # Tokenize batch
        inputs = tokenizer(
            prompts,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=2048
        )
        
        # å°†è¾“å…¥ç§»åˆ°ç¬¬ä¸€ä¸ªGPUï¼ˆDeepSpeedä¼šå¤„ç†åˆ†å¸ƒï¼‰
        input_ids = inputs.input_ids.to(f"cuda:{ds_engine.local_rank}")
        attention_mask = inputs.attention_mask.to(f"cuda:{ds_engine.local_rank}")
        
        # ä½¿ç”¨DeepSpeedè¿›è¡Œæ¨ç†
        with torch.no_grad():
            outputs = ds_engine.module.generate(
                input_ids=input_ids,
                attention_mask=attention_mask,
                max_new_tokens=32,
                temperature=0.1,
                top_p=0.95,
                do_sample=True,
                pad_token_id=tokenizer.pad_token_id,
                eos_token_id=tokenizer.eos_token_id
            )
        
        # è§£ç è¾“å‡º
        results = []
        valid_labels = ['æ­£å¸¸', 'æ”¿æ²»å®‰å…¨', 'è‰²æƒ…ä½ä¿—', 'è¿æ³•è¿è§„', 'æš´æ', 'æ­§è§†']
        
        for i, output in enumerate(outputs):
            # å»é™¤è¾“å…¥éƒ¨åˆ†ï¼Œåªä¿ç•™ç”Ÿæˆçš„å†…å®¹
            generated_ids = output[len(input_ids[i]):]
            generated_text = tokenizer.decode(generated_ids, skip_special_tokens=True).strip()
            
            # æå–æ ‡ç­¾
            found_label = "æ­£å¸¸"  # é»˜è®¤æ ‡ç­¾
            for label in valid_labels:
                if label in generated_text:
                    found_label = label
                    break
            
            results.append(found_label)
        
        return results
        
    except Exception as e:
        print(f"DeepSpeedæ¨¡å‹æ‰¹é‡è°ƒç”¨é”™è¯¯: {e}")
        return ["æ­£å¸¸"] * len(texts)  # é»˜è®¤è¿”å›æ­£å¸¸

def analyze_disagreement(original_label, predicted_label, verification_label):
    """åˆ†æå½“åŸå§‹æ ‡ç­¾å’Œé¢„æµ‹æ ‡ç­¾ä¸ä¸€è‡´æ—¶ï¼Œ32Bæ¨¡å‹çš„åˆ¤æ–­å€¾å‘"""
    if original_label == predicted_label:
        return "ä¸€è‡´", "æ— éœ€æ¯”è¾ƒ"
    
    # åŸå§‹æ ‡ç­¾å’Œé¢„æµ‹æ ‡ç­¾ä¸ä¸€è‡´çš„æƒ…å†µ
    if verification_label == original_label:
        return "ä¸ä¸€è‡´", "æ”¯æŒåŸå§‹æ ‡ç­¾"
    elif verification_label == predicted_label:
        return "ä¸ä¸€è‡´", "æ”¯æŒé¢„æµ‹æ ‡ç­¾"
    else:
        return "ä¸ä¸€è‡´", "éƒ½ä¸æ”¯æŒ"

def main():
    # åˆå§‹åŒ–DeepSpeed
    if not initialize_deepspeed_model():
        print("DeepSpeedåˆå§‹åŒ–å¤±è´¥ï¼Œé€€å‡ºç¨‹åº")
        return
    
    # è¯»å–CSVæ•°æ®æ–‡ä»¶
    data_file = './detailed_predictions_20250709_112207.csv'
    print("è¯»å–CSVæ•°æ®æ–‡ä»¶...")
    
    try:
        df = read_csv_data(data_file)
        if df is None:
            print(f"æ— æ³•è¯»å–æ–‡ä»¶ {data_file}")
            return
    except FileNotFoundError:
        print(f"æ–‡ä»¶ {data_file} ä¸å­˜åœ¨")
        return
    
    # æ£€æŸ¥å¿…è¦çš„åˆ—æ˜¯å¦å­˜åœ¨
    # æ¨æµ‹å¯èƒ½çš„åˆ—åï¼ˆè€ƒè™‘ä¸­è‹±æ–‡å’Œä¸åŒçš„å‘½åæ–¹å¼ï¼‰
    text_column = None
    original_label_column = None
    predicted_label_column = None
    
    for col in df.columns:
        if 'åŸå§‹æ–‡æœ¬' in col or 'æ–‡æœ¬' in col or 'text' in col.lower():
            text_column = col
        elif 'åŸå§‹æ ‡ç­¾' in col or 'true' in col.lower() or 'original' in col.lower():
            original_label_column = col
        elif 'é¢„æµ‹æ ‡ç­¾' in col or 'pred' in col.lower() or 'predict' in col.lower():
            predicted_label_column = col
    
    if text_column is None:
        print(f"æœªæ‰¾åˆ°æ–‡æœ¬åˆ—ï¼Œå¯ç”¨åˆ—å: {list(df.columns)}")
        return
    
    print(f"ä½¿ç”¨æ–‡æœ¬åˆ—: {text_column}")
    print(f"åŸå§‹æ ‡ç­¾åˆ—: {original_label_column}")
    print(f"é¢„æµ‹æ ‡ç­¾åˆ—: {predicted_label_column}")
    
    total = len(df)
    print(f"å¼€å§‹å¤„ç† {total} æ¡æ•°æ®...")
    
    # æ‰¹é‡å¤„ç† - DeepSpeedä¼˜åŒ–çš„æ‰¹æ¬¡å¤§å°
    # æ ¹æ®GPUæ•°é‡å’Œå†…å­˜æƒ…å†µè°ƒæ•´ï¼ŒDeepSpeedå¯ä»¥å¤„ç†æ›´å¤§çš„æ‰¹æ¬¡
    batch_size = 32 * actual_gpu_count  # DeepSpeedä¸‹çš„æ‰¹æ¬¡å¤§å°
    print(f"ä½¿ç”¨DeepSpeedæ‰¹å¤„ç†å¤§å°: {batch_size}")
    
    # å­˜å‚¨ç»“æœ
    verification_labels = []
    
    # åˆ›å»ºè¿›åº¦æ¡
    progress_bar = tqdm(range(0, total, batch_size),
                       desc="DeepSpeed 32Bæ¨¡å‹æ£€éªŒè¿›åº¦",
                       unit="batch",
                       ncols=120,
                       bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [å·²å¤„ç†: {postfix}]')
    
    processed_count = 0
    import time
    start_time = time.time()
    
    for i in progress_bar:
        batch_start_time = time.time()
        batch_df = df.iloc[i:i+batch_size]
        texts = batch_df[text_column].astype(str).tolist()
        
        # ä½¿ç”¨DeepSpeedæ¨¡å‹æ‰¹é‡åˆ†ç±»
        batch_verification_labels = classify_batch_deepspeed(texts)
        verification_labels.extend(batch_verification_labels)
        
        processed_count += len(batch_df)
        batch_time = time.time() - batch_start_time
        avg_time_per_sample = batch_time / len(batch_df)
        throughput = len(batch_df) / batch_time
        
        # æ›´æ–°è¿›åº¦æ¡æ˜¾ç¤ºä¿¡æ¯
        progress_bar.set_postfix({
            'æ ·æœ¬': f'{processed_count}/{total}',
            'æ‰¹æ¬¡å¤§å°': len(batch_df),
            'ååé‡': f'{throughput:.1f}æ ·æœ¬/ç§’',
            'æ‰¹æ¬¡è€—æ—¶': f'{batch_time:.1f}ç§’'
        })
    
    progress_bar.close()
    
    # æ·»åŠ 32Bæ¨¡å‹æ£€éªŒæ ‡ç­¾åˆ—
    df['32Bæ¨¡å‹æ£€éªŒæ ‡ç­¾'] = verification_labels
    
    # åˆ†æä¸ä¸€è‡´æƒ…å†µï¼ˆå¦‚æœæœ‰åŸå§‹æ ‡ç­¾å’Œé¢„æµ‹æ ‡ç­¾åˆ—ï¼‰
    disagreement_analysis = {
        'æ”¯æŒåŸå§‹æ ‡ç­¾': 0,
        'æ”¯æŒé¢„æµ‹æ ‡ç­¾': 0, 
        'éƒ½ä¸æ”¯æŒ': 0,
        'æ€»ä¸ä¸€è‡´æ•°': 0
    }
    
    if original_label_column and predicted_label_column:
        for idx, row in df.iterrows():
            original = row[original_label_column]
            predicted = row[predicted_label_column]
            verification = row['32Bæ¨¡å‹æ£€éªŒæ ‡ç­¾']
            
            consistency, support = analyze_disagreement(original, predicted, verification)
            
            if consistency == "ä¸ä¸€è‡´":
                disagreement_analysis['æ€»ä¸ä¸€è‡´æ•°'] += 1
                if support == "æ”¯æŒåŸå§‹æ ‡ç­¾":
                    disagreement_analysis['æ”¯æŒåŸå§‹æ ‡ç­¾'] += 1
                elif support == "æ”¯æŒé¢„æµ‹æ ‡ç­¾":
                    disagreement_analysis['æ”¯æŒé¢„æµ‹æ ‡ç­¾'] += 1
                elif support == "éƒ½ä¸æ”¯æŒ":
                    disagreement_analysis['éƒ½ä¸æ”¯æŒ'] += 1
    
    # ä¿å­˜ç»“æœ
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    output_filename = f"zzz/enhanced_with_deepspeed_verification_{timestamp}.csv"
    
    # ä¿å­˜å¢å¼ºçš„CSVæ–‡ä»¶
    df.to_csv(output_filename, index=False, encoding='utf-8-sig')
    print(f"âœ… DeepSpeedå¢å¼ºçš„CSVæ–‡ä»¶å·²ä¿å­˜: {output_filename}")
    
    # è¾“å‡ºç»Ÿè®¡ç»“æœ
    print(f"\n=== DeepSpeed 32Bæ¨¡å‹æ£€éªŒç»“æœç»Ÿè®¡ ===")
    print(f"æ€»æ ·æœ¬æ•°: {total}")
    print(f"DeepSpeed 32Bæ¨¡å‹æ£€éªŒå®Œæˆ")
    
    if original_label_column and predicted_label_column:
        print(f"\n=== ä¸ä¸€è‡´æƒ…å†µåˆ†æ ===")
        print(f"åŸå§‹æ ‡ç­¾ä¸é¢„æµ‹æ ‡ç­¾ä¸ä¸€è‡´çš„æ ·æœ¬æ•°: {disagreement_analysis['æ€»ä¸ä¸€è‡´æ•°']}")
        
        if disagreement_analysis['æ€»ä¸ä¸€è‡´æ•°'] > 0:
            print(f"åœ¨ä¸ä¸€è‡´çš„æƒ…å†µä¸‹ï¼š")
            print(f"  32Bæ¨¡å‹æ”¯æŒåŸå§‹æ ‡ç­¾: {disagreement_analysis['æ”¯æŒåŸå§‹æ ‡ç­¾']} ({disagreement_analysis['æ”¯æŒåŸå§‹æ ‡ç­¾']/disagreement_analysis['æ€»ä¸ä¸€è‡´æ•°']:.1%})")
            print(f"  32Bæ¨¡å‹æ”¯æŒé¢„æµ‹æ ‡ç­¾: {disagreement_analysis['æ”¯æŒé¢„æµ‹æ ‡ç­¾']} ({disagreement_analysis['æ”¯æŒé¢„æµ‹æ ‡ç­¾']/disagreement_analysis['æ€»ä¸ä¸€è‡´æ•°']:.1%})")
            print(f"  32Bæ¨¡å‹éƒ½ä¸æ”¯æŒ: {disagreement_analysis['éƒ½ä¸æ”¯æŒ']} ({disagreement_analysis['éƒ½ä¸æ”¯æŒ']/disagreement_analysis['æ€»ä¸ä¸€è‡´æ•°']:.1%})")
        
        # è®¡ç®—32Bæ¨¡å‹ä¸åŸå§‹æ ‡ç­¾ã€é¢„æµ‹æ ‡ç­¾çš„ä¸€è‡´æ€§
        if original_label_column:
            original_agreement = sum(1 for i in range(len(df)) if df.iloc[i][original_label_column] == df.iloc[i]['32Bæ¨¡å‹æ£€éªŒæ ‡ç­¾'])
            original_agreement_rate = original_agreement / total
            print(f"\n32Bæ¨¡å‹ä¸åŸå§‹æ ‡ç­¾ä¸€è‡´ç‡: {original_agreement_rate:.2%} ({original_agreement}/{total})")
        
        if predicted_label_column:
            predicted_agreement = sum(1 for i in range(len(df)) if df.iloc[i][predicted_label_column] == df.iloc[i]['32Bæ¨¡å‹æ£€éªŒæ ‡ç­¾'])
            predicted_agreement_rate = predicted_agreement / total
            print(f"32Bæ¨¡å‹ä¸é¢„æµ‹æ ‡ç­¾ä¸€è‡´ç‡: {predicted_agreement_rate:.2%} ({predicted_agreement}/{total})")
    
    # ç»Ÿè®¡å„æ ‡ç­¾åˆ†å¸ƒ
    print(f"\n=== DeepSpeed 32Bæ¨¡å‹æ£€éªŒæ ‡ç­¾åˆ†å¸ƒ ===")
    label_counts = df['32Bæ¨¡å‹æ£€éªŒæ ‡ç­¾'].value_counts()
    for label, count in label_counts.items():
        print(f"{label}: {count} ({count/total:.1%})")
    
    # ä¿å­˜è¯¦ç»†ç»Ÿè®¡æŠ¥å‘Š
    report_data = {
        'timestamp': timestamp,
        'total_samples': total,
        'disagreement_analysis': disagreement_analysis,
        'label_distribution': label_counts.to_dict(),
        'acceleration_method': 'DeepSpeed'
    }
    
    if original_label_column and predicted_label_column:
        report_data['original_agreement_rate'] = original_agreement_rate
        report_data['predicted_agreement_rate'] = predicted_agreement_rate
    
    report_filename = f"zzz/deepspeed_verification_report_{timestamp}.json"
    with open(report_filename, 'w', encoding='utf-8') as f:
        json.dump(report_data, f, ensure_ascii=False, indent=2)
    
    print(f"\n=== æ–‡ä»¶ä¿å­˜æ€»ç»“ ===")
    print(f"ğŸ“„ DeepSpeedå¢å¼ºçš„CSVæ–‡ä»¶: {output_filename}")
    print(f"   - åŸæœ‰åˆ— + '32Bæ¨¡å‹æ£€éªŒæ ‡ç­¾'åˆ—")
    print(f"ğŸ“‹ DeepSpeedç»Ÿè®¡æŠ¥å‘ŠJSON: {report_filename}")
    print(f"   - è¯¦ç»†çš„ä¸€è‡´æ€§å’Œåˆ†å¸ƒç»Ÿè®¡")

if __name__ == "__main__":
    # åˆå§‹åŒ–åˆ†å¸ƒå¼ç¯å¢ƒ
    import deepspeed
    deepspeed.init_distributed()
    
    main()