from datetime import datetime
from tqdm import tqdm
import json
from vllm import LLM, SamplingParams
from sklearn.metrics import classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import openpyxl
from openpyxl import Workbook

# åˆå§‹åŒ–vLLMæ¨¡å‹ - éƒ¨åˆ†GPUå¹¶è¡Œé…ç½®
import os
import logging
import sys

# ç¦ç”¨vLLMçš„è¯¦ç»†æ—¥å¿—è¾“å‡ºå’Œè¿›åº¦æ¡
logging.getLogger("vllm").setLevel(logging.CRITICAL)
logging.getLogger("transformers").setLevel(logging.CRITICAL)
logging.getLogger("torch").setLevel(logging.CRITICAL)
os.environ["TOKENIZERS_PARALLELISM"] = "false"
os.environ["VLLM_LOGGING_LEVEL"] = "CRITICAL"
os.environ["CUDA_LAUNCH_BLOCKING"] = "0"

# ç¦ç”¨æ‰€æœ‰ä¸å¿…è¦çš„è¾“å‡º
import warnings
warnings.filterwarnings("ignore")

# é‡å®šå‘stderræ¥ç¦ç”¨vLLMå†…éƒ¨è¿›åº¦æ¡
import contextlib
import io

class SuppressOutput:
    def __init__(self):
        self._original_stderr = sys.stderr
        
    def __enter__(self):
        sys.stderr = io.StringIO()
        return self
        
    def __exit__(self, exc_type, exc_val, exc_tb):
        sys.stderr = self._original_stderr

# ==================== GPUé…ç½® ====================
# è®¾ç½®è¦ä½¿ç”¨çš„GPUæ•°é‡ï¼ˆæ ¹æ®éœ€è¦ä¿®æ”¹ï¼‰
USE_GPU_COUNT = 4  # æŒ‡å®šä½¿ç”¨çš„GPUæ•°é‡ï¼Œä¾‹å¦‚ï¼šåªä½¿ç”¨2ä¸ªGPU

# è®¾ç½®å¯è§çš„GPUè®¾å¤‡ï¼ˆå¯é€‰ï¼‰
os.environ["CUDA_VISIBLE_DEVICES"] = "0,1,2,3"  # åªä½¿ç”¨GPU 0å’Œ1

# æ£€æµ‹å¯ç”¨GPUæ•°é‡
def get_gpu_count():
    try:
        import torch
        return torch.cuda.device_count()
    except:
        return 1

available_gpu_count = get_gpu_count()
actual_gpu_count = min(USE_GPU_COUNT, available_gpu_count)

print(f"ç³»ç»Ÿå¯ç”¨GPUæ•°é‡: {available_gpu_count}")
print(f"é…ç½®ä½¿ç”¨GPUæ•°é‡: {actual_gpu_count}")

try:
    # éƒ¨åˆ†GPUå¹¶è¡Œé…ç½®
    llm = LLM(
        model="/home/users/sx_zhuzz/folder/llms-from-scratch/Qwen3",
        tensor_parallel_size=actual_gpu_count,  # ä½¿ç”¨æŒ‡å®šæ•°é‡çš„GPU
        gpu_memory_utilization=0.7,  # å¤šå¡æƒ…å†µä¸‹å¯ä»¥ä½¿ç”¨æ›´é«˜çš„å†…å­˜åˆ©ç”¨ç‡
        max_model_len=8192,  # å¢åŠ æœ€å¤§é•¿åº¦ä»¥æ”¯æŒæ›´é•¿çš„prompt
        dtype="half",  # ä½¿ç”¨åŠç²¾åº¦
        trust_remote_code=True,  # å¦‚æœæ¨¡å‹éœ€è¦è‡ªå®šä¹‰ä»£ç 
        enforce_eager=False  # å¤šå¡æ—¶é€šå¸¸å¯ä»¥å¯ç”¨CUDAå›¾
    )
    sampling_params = SamplingParams(temperature=0.1, top_p=0.95, max_tokens=32)
    print(f"æˆåŠŸä½¿ç”¨ {actual_gpu_count} ä¸ªGPUåŠ è½½æ¨¡å‹ï¼")
    
except Exception as e:
    print(f"æŒ‡å®šGPUæ•°é‡é…ç½®å¤±è´¥: {e}")
    print("å°è¯•ä½¿ç”¨æ›´ä¿å®ˆçš„é…ç½®...")
    
    # å¤‡ç”¨æ–¹æ¡ˆ1ï¼šå‡å°‘GPUæ•°é‡
    try:
        backup_gpu_count = max(1, actual_gpu_count // 2)
        llm = LLM(
            model="/home/users/sx_zhuzz/folder/llms-from-scratch/Qwen3",
            tensor_parallel_size=backup_gpu_count,
            gpu_memory_utilization=0.8,
            max_model_len=4096,
            dtype="half",
            trust_remote_code=True,
            enforce_eager=True
        )
        sampling_params = SamplingParams(temperature=0.1, top_p=0.95, max_tokens=32)
        print(f"ä½¿ç”¨å¤‡ç”¨é…ç½®æˆåŠŸåŠ è½½æ¨¡å‹ï¼ˆ{backup_gpu_count} GPUï¼‰ï¼")
        
    except Exception as e2:
        print(f"å¤‡ç”¨é…ç½®ä¹Ÿå¤±è´¥äº†: {e2}")
        print("å°è¯•å•å¡é…ç½®...")
        
        # å¤‡ç”¨æ–¹æ¡ˆ2ï¼šå•å¡é…ç½®
        try:
            llm = LLM(
                model="/home/users/sx_zhuzz/folder/llms-from-scratch/Qwen3",
                tensor_parallel_size=1,
                gpu_memory_utilization=0.7,
                max_model_len=4096,  # å¢åŠ æœ€å¤§é•¿åº¦
                dtype="half",
                trust_remote_code=True,
                enforce_eager=True
            )
            sampling_params = SamplingParams(temperature=0.1, top_p=0.95, max_tokens=32)
            print("ä½¿ç”¨å•å¡é…ç½®åŠ è½½æ¨¡å‹æˆåŠŸï¼")
        except Exception as e3:
            print(f"æ‰€æœ‰é…ç½®éƒ½å¤±è´¥äº†: {e3}")
            print("å»ºè®®ï¼š")
            print("1. æ£€æŸ¥GPUçŠ¶æ€ï¼šnvidia-smi")
            print("2. æ¸…ç†GPUå†…å­˜ï¼špkill -f python")
            print("3. æ£€æŸ¥æ¨¡å‹è·¯å¾„æ˜¯å¦æ­£ç¡®")
            print("4. è®¾ç½®ç¯å¢ƒå˜é‡ï¼šexport CUDA_VISIBLE_DEVICES=0,1")
            exit(1)

def read_json_data(file_path: str) -> list:
    """è¯»å–JSONæ•°æ®æ–‡ä»¶ï¼Œè‡ªåŠ¨æ£€æµ‹ç¼–ç æ ¼å¼"""
    data = []
    
    # å°è¯•å¤šç§ç¼–ç æ ¼å¼
    encodings = ['utf-8', 'gbk', 'gb2312', 'utf-8-sig', 'latin1']
    
    for encoding in encodings:
        try:
            print(f"å°è¯•ä½¿ç”¨ {encoding} ç¼–ç è¯»å–æ–‡ä»¶...")
            with open(file_path, 'r', encoding=encoding) as file:
                content = file.read().strip()
                
                # å°è¯•æŒ‰è¡Œè¯»å–ï¼ˆå¦‚æœæ˜¯JSONLæ ¼å¼ï¼‰
                if content.startswith('{'):
                    for line in content.split('\n'):
                        if line.strip():
                            try:
                                item = json.loads(line.strip())
                                data.append(item)
                            except json.JSONDecodeError:
                                continue
                else:
                    # å°è¯•ä½œä¸ºå®Œæ•´JSONæ•°ç»„è¯»å–
                    try:
                        data = json.loads(content)
                    except json.JSONDecodeError as e:
                        print(f"JSONè§£æé”™è¯¯: {e}")
                        return []
                
                print(f"æˆåŠŸä½¿ç”¨ {encoding} ç¼–ç è¯»å–æ–‡ä»¶ï¼Œå…±è¯»å– {len(data)} æ¡æ•°æ®")
                return data
                
        except UnicodeDecodeError:
            print(f"{encoding} ç¼–ç å¤±è´¥ï¼Œå°è¯•ä¸‹ä¸€ç§ç¼–ç ...")
            continue
        except Exception as e:
            print(f"ä½¿ç”¨ {encoding} ç¼–ç æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            continue
    
    # å¦‚æœæ‰€æœ‰ç¼–ç éƒ½å¤±è´¥ï¼Œå°è¯•äºŒè¿›åˆ¶è¯»å–å¹¶æ£€æµ‹ç¼–ç 
    try:
        import chardet
        with open(file_path, 'rb') as file:
            raw_data = file.read()
            detected = chardet.detect(raw_data)
            detected_encoding = detected['encoding']
            confidence = detected['confidence']
            
            print(f"æ£€æµ‹åˆ°æ–‡ä»¶ç¼–ç : {detected_encoding} (ç½®ä¿¡åº¦: {confidence:.2f})")
            
            if detected_encoding and confidence > 0.7:
                try:
                    content = raw_data.decode(detected_encoding).strip()
                    
                    # å¤„ç†JSONLæ ¼å¼
                    if content.startswith('{'):
                        for line in content.split('\n'):
                            if line.strip():
                                try:
                                    item = json.loads(line.strip())
                                    data.append(item)
                                except json.JSONDecodeError:
                                    continue
                    else:
                        # å¤„ç†JSONæ•°ç»„æ ¼å¼
                        try:
                            data = json.loads(content)
                        except json.JSONDecodeError as e:
                            print(f"JSONè§£æé”™è¯¯: {e}")
                            return []
                    
                    print(f"æˆåŠŸä½¿ç”¨æ£€æµ‹åˆ°çš„ç¼–ç  {detected_encoding} è¯»å–æ–‡ä»¶ï¼Œå…±è¯»å– {len(data)} æ¡æ•°æ®")
                    return data
                    
                except Exception as e:
                    print(f"ä½¿ç”¨æ£€æµ‹åˆ°çš„ç¼–ç  {detected_encoding} å¤±è´¥: {e}")
            
    except ImportError:
        print("chardetåº“æœªå®‰è£…ï¼Œæ— æ³•è‡ªåŠ¨æ£€æµ‹ç¼–ç ")
    except Exception as e:
        print(f"è‡ªåŠ¨ç¼–ç æ£€æµ‹å¤±è´¥: {e}")
    
    print("æ‰€æœ‰ç¼–ç å°è¯•éƒ½å¤±è´¥äº†ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶æ ¼å¼æˆ–æ‰‹åŠ¨æŒ‡å®šæ­£ç¡®çš„ç¼–ç ")
    return []

def classify_text(text: str) -> str:
    """ä½¿ç”¨vLLMæ¨¡å‹å¯¹æ–‡æœ¬è¿›è¡Œåˆ†ç±»"""
    prompt = f"""ä½ æ˜¯ä¸€ä¸ªæ–‡æœ¬å†…å®¹åˆè§„å®¡æ ¸å‘˜ï¼Œä½ çš„ä»»åŠ¡æ˜¯æ ¹æ®å¹³å°çš„è¯„åˆ¤å°ºåº¦è§„åˆ™åˆ¤æ–­ç”¨æˆ·çš„è¾“å…¥æ˜¯å¦åˆè§„ï¼Œä»”ç»†é˜…è¯»ä¸‹é¢çš„è¯„åˆ¤å°ºåº¦è§„åˆ™ã€‚

##è¯„åˆ¤å°ºåº¦è§„åˆ™##
#æ”¿æ²»å®‰å…¨#
è¿è§„èŒƒå›´ï¼š
1. ä¸‘åŒ–ã€è¯½è°¤å…šå’Œå›½å®¶é¢†å¯¼äººï¼ŒåŒ…æ‹¬ä½¿ç”¨ä¾®è¾±æ€§ç»°å·æˆ–å½±å°„è¯­è¨€ã€‚
2. é¼“åŠ¨é¢ è¦†å›½å®¶æ”¿æƒã€ä¼ æ’­åˆ†è£‚å›½å®¶ã€æ°‘æ—ä»‡æ¨ç­‰å†…å®¹ã€‚
3. å®£æ‰¬å¢ƒå¤–åååŠ¿åŠ›ã€é‚ªæ•™ç»„ç»‡ï¼Œæˆ–ç…½åŠ¨å¯¹æŠ—æ”¿åºœè¨€è®ºã€‚
4. æ¶‰åŠæ”¿æ²»æ•æ„Ÿè¯é¢˜ã€é¢ è¦†æ€§è¨€è®ºã€‚
è¿è§„æ¡ˆä¾‹ï¼š
"ä¹ åŒ…å­æ¯äº†è¿™ä¸ªå›½å®¶ã€‚" "é¦™æ¸¯å¿…é¡»ç‹¬ç«‹ï¼Œä¸­å›½æ”¿åºœæ˜¯ç‹¬è£è€…ã€‚"

#è‰²æƒ…ä½ä¿—#
è¿è§„èŒƒå›´ï¼š
1. æ¶‰åŠæ€§è¡Œä¸ºæå†™ã€æ€§æš—ç¤ºã€è£¸éœ²å†…å®¹ï¼Œæˆ–æ·«ç§½ã€ä½ä¿—è¨€è¾ã€‚
2. æ¨å¹¿ã€ä¼ æ’­è‰²æƒ…å†…å®¹ã€é“¾æ¥ã€å›¾æ–‡æˆ–è§†é¢‘ã€‚
3. å«æœ‰æŒ‘é€—ã€çŒ¥äºµã€è‰²æƒ…ç­‰ä¸å½“å†…å®¹ã€‚
è¿è§„æ¡ˆä¾‹ï¼š
"é«˜æ¸…æ— ç åˆé›†ä¸‹è½½ï¼Œç§å¯†è§†é¢‘æµå‡ºã€‚" "æè¿°æ€§è¡Œä¸ºè¿‡ç¨‹çš„æ–‡ç« æˆ–ç‰‡æ®µã€‚"

#è¿æ³•è¿è§„#
è¿è§„èŒƒå›´ï¼š
1. å®£ä¼ ã€å¼•å¯¼å‚ä¸èµŒåšè¡Œä¸ºï¼ŒåŒ…æ‹¬èµŒçƒã€çº¿ä¸ŠèµŒåšç­‰ã€‚
2. æä¾›åšå½©ç½‘ç«™ã€é‚€è¯·ç ã€ä¸‹æ³¨æŠ€å·§ç­‰ä¿¡æ¯ã€‚
3. é¼“åŠ±ã€æ•™å”†ä»–äººå¸æ¯’ï¼Œæˆ–æå†™å¸æ¯’è¿‡ç¨‹ã€‚
4. è´©æ¯’å¼•æµã€ä»‹ç»æ¯’å“è·å–æ¸ é“ã€‚
5. å…¶ä»–è¿æ³•çŠ¯ç½ªç›¸å…³å†…å®¹ã€‚
è¿è§„æ¡ˆä¾‹ï¼š
"æƒ³èµšé’±å°±ä¸Šè¿™ä¸ªå¹³å°ï¼Œç¨³èµšä¸èµ”ï¼" "è¯•è¯•è¿™ç©æ„ï¼Œèƒ½è®©ä½ é£èµ·æ¥ã€‚"

#æš´æ#
è¿è§„èŒƒå›´ï¼š
1. é¼“å¹æš´åŠ›æç«¯æ€æƒ³ã€ä»‡æ¨ç…½åŠ¨å†…å®¹ã€‚
2. å¨èƒå®æ–½ææ€–è¡ŒåŠ¨ã€åˆ¶é€ ææ…Œæˆ–ç…½åŠ¨ä»–äººæè¢­ã€‚
3. æ¶‰åŠæš´åŠ›ã€ææ€–ä¸»ä¹‰ã€æç«¯ä¸»ä¹‰ç­‰å±é™©å†…å®¹ã€‚
è¿è§„æ¡ˆä¾‹ï¼š
"æˆ‘ä»¬è¦ç‚¸æ‰åœ°é“ç«™ï¼" "åŠ å…¥åœ£æˆ˜å§ï¼Œè¿™æ˜¯ç¥çš„æ—¨æ„ã€‚"

#æ­§è§†#
è¿è§„èŒƒå›´ï¼š
1. åŸºäºç§æ—ã€æ€§åˆ«ã€å®—æ•™ã€å›½ç±ç­‰çš„æ­§è§†æ€§è¨€è®ºã€‚
2. ç…½åŠ¨ä»‡æ¨ã€åè§æˆ–æ­§è§†çš„å†…å®¹ã€‚
3. ä¾®è¾±ã€è´¬ä½ç‰¹å®šç¾¤ä½“çš„è¨€è®ºã€‚
è¿è§„æ¡ˆä¾‹ï¼š
"æŸæŸæ°‘æ—å°±æ˜¯åŠ£ç­‰çš„ã€‚" "å¥³æ€§å¤©ç”Ÿå°±ä¸é€‚åˆå·¥ä½œã€‚"

#æ­£å¸¸#
æ­£å¸¸èŒƒå›´ï¼š
1. ç”Ÿæ´»ã€å­¦ä¹ ã€å·¥ä½œã€æƒ…æ„Ÿã€æ–°é—»ã€å¨±ä¹ç­‰æ™®é€šå†…å®¹ã€‚
2. ä¸æ¶‰åŠä¸Šè¿°ä»»ä½•è¿è§„å†…å®¹çš„æ­£å½“è¡¨è¾¾ã€‚
3. å®¢è§‚ã€ä¸­æ€§çš„ä¿¡æ¯åˆ†äº«å’Œè®¨è®ºã€‚
æ­£å¸¸æ¡ˆä¾‹ï¼š
"ä»Šå¤©é˜³å…‰çœŸå¥½ï¼Œé€‚åˆå»å…¬å›­æ•£æ­¥ã€‚" "æˆ‘åªæƒ³æ‘†è„±è¿™äº›ç—›è‹¦ï¼Œéœ€è¦ä¸€äº›å¸®åŠ©ã€‚"


##åˆ¤æ–­è¿‡ç¨‹##
ä¸¥æ ¼æŒ‰ç…§ä¸Šè¿°è§„åˆ™ï¼Œä¸€ä¸ªæ ‡ç­¾ä¸€ä¸ªæ ‡ç­¾åˆ¤æ–­ã€‚


##è¾“å‡ºè¦æ±‚##
åªèƒ½è¾“å‡º'æ­£å¸¸'æˆ–'æ”¿æ²»å®‰å…¨'æˆ–'è‰²æƒ…ä½ä¿—'æˆ–'è¿æ³•è¿è§„'æˆ–'æš´æ'æˆ–'æ­§è§†'è¿™6ä¸ªæ ‡ç­¾ä¸­çš„ä¸€ä¸ªï¼Œä¸è¦è¾“å‡ºä»»ä½•å…¶ä»–é¢å¤–çš„ä¿¡æ¯æˆ–è§£é‡Šã€‚æ³¨æ„ï¼Œä½ è¦åˆ¤æ–­ã€ã€‘ä¸­æ–‡æœ¬å†…å®¹æ˜¯å¦è¿è§„å¹¶ç»™å‡ºå¯¹åº”çš„æ ‡ç­¾ï¼Œè€Œä¸æ˜¯å›ç­”æ–‡æœ¬çš„é—®é¢˜ã€‚

ã€{text}ã€‘

è¯·åˆ¤æ–­ä¸Šè¿°æ–‡æœ¬çš„æ ‡ç­¾ï¼š"""
    
    try:
        outputs = llm.generate([prompt], sampling_params)
        raw_output = outputs[0].outputs[0].text.strip()
        
        # æå–æ ‡ç­¾
        valid_labels = ['æ­£å¸¸', 'æ”¿æ²»å®‰å…¨', 'è‰²æƒ…ä½ä¿—', 'è¿æ³•è¿è§„', 'æš´æ', 'æ­§è§†']
        for label in valid_labels:
            if label in raw_output:
                return label
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆæ ‡ç­¾ï¼Œé»˜è®¤è¿”å›æ­£å¸¸
        return "æ­£å¸¸"
        
    except Exception as e:
        print(f"vLLMè°ƒç”¨é”™è¯¯: {e}")
        return "æ­£å¸¸"  # é»˜è®¤è¿”å›æ­£å¸¸

def classify_batch(texts: list) -> list:
    """æ‰¹é‡åˆ†ç±»æ–‡æœ¬ä»¥æé«˜æ•ˆç‡"""
    prompts = []
    for text in texts:
        prompt = f"""ä½ æ˜¯ä¸€ä¸ªæ–‡æœ¬å†…å®¹åˆè§„å®¡æ ¸å‘˜ï¼Œä½ çš„ä»»åŠ¡æ˜¯æ ¹æ®å¹³å°çš„è¯„åˆ¤å°ºåº¦è§„åˆ™åˆ¤æ–­ç”¨æˆ·çš„è¾“å…¥æ˜¯å¦åˆè§„ï¼Œä»”ç»†é˜…è¯»ä¸‹é¢çš„è¯„åˆ¤å°ºåº¦è§„åˆ™ã€‚

##è¯„åˆ¤å°ºåº¦è§„åˆ™##
#æ”¿æ²»å®‰å…¨#
è¿è§„èŒƒå›´ï¼š
1. ä¸‘åŒ–ã€è¯½è°¤å…šå’Œå›½å®¶é¢†å¯¼äººï¼ŒåŒ…æ‹¬ä½¿ç”¨ä¾®è¾±æ€§ç»°å·æˆ–å½±å°„è¯­è¨€ã€‚
2. é¼“åŠ¨é¢ è¦†å›½å®¶æ”¿æƒã€ä¼ æ’­åˆ†è£‚å›½å®¶ã€æ°‘æ—ä»‡æ¨ç­‰å†…å®¹ã€‚
3. å®£æ‰¬å¢ƒå¤–åååŠ¿åŠ›ã€é‚ªæ•™ç»„ç»‡ï¼Œæˆ–ç…½åŠ¨å¯¹æŠ—æ”¿åºœè¨€è®ºã€‚
4. æ¶‰åŠæ”¿æ²»æ•æ„Ÿè¯é¢˜ã€é¢ è¦†æ€§è¨€è®ºã€‚

#è‰²æƒ…ä½ä¿—#
è¿è§„èŒƒå›´ï¼š
1. æ¶‰åŠæ€§è¡Œä¸ºæå†™ã€æ€§æš—ç¤ºã€è£¸éœ²å†…å®¹ï¼Œæˆ–æ·«ç§½ã€ä½ä¿—è¨€è¾ã€‚
2. æ¨å¹¿ã€ä¼ æ’­è‰²æƒ…å†…å®¹ã€é“¾æ¥ã€å›¾æ–‡æˆ–è§†é¢‘ã€‚
3. å«æœ‰æŒ‘é€—ã€çŒ¥äºµã€è‰²æƒ…ç­‰ä¸å½“å†…å®¹ã€‚

#è¿æ³•è¿è§„#
è¿è§„èŒƒå›´ï¼š
1. å®£ä¼ ã€å¼•å¯¼å‚ä¸èµŒåšè¡Œä¸ºï¼ŒåŒ…æ‹¬èµŒçƒã€çº¿ä¸ŠèµŒåšç­‰ã€‚
2. æä¾›åšå½©ç½‘ç«™ã€é‚€è¯·ç ã€ä¸‹æ³¨æŠ€å·§ç­‰ä¿¡æ¯ã€‚
3. é¼“åŠ±ã€æ•™å”†ä»–äººå¸æ¯’ï¼Œæˆ–æå†™å¸æ¯’è¿‡ç¨‹ã€‚
4. è´©æ¯’å¼•æµã€ä»‹ç»æ¯’å“è·å–æ¸ é“ã€‚
5. å…¶ä»–è¿æ³•çŠ¯ç½ªç›¸å…³å†…å®¹ã€‚

#æš´æ#
è¿è§„èŒƒå›´ï¼š
1. é¼“å¹æš´åŠ›æç«¯æ€æƒ³ã€ä»‡æ¨ç…½åŠ¨å†…å®¹ã€‚
2. å¨èƒå®æ–½ææ€–è¡ŒåŠ¨ã€åˆ¶é€ ææ…Œæˆ–ç…½åŠ¨ä»–äººæè¢­ã€‚
3. æ¶‰åŠæš´åŠ›ã€ææ€–ä¸»ä¹‰ã€æç«¯ä¸»ä¹‰ç­‰å±é™©å†…å®¹ã€‚

#æ­§è§†#
è¿è§„èŒƒå›´ï¼š
1. åŸºäºç§æ—ã€æ€§åˆ«ã€å®—æ•™ã€å›½ç±ç­‰çš„æ­§è§†æ€§è¨€è®ºã€‚
2. ç…½åŠ¨ä»‡æ¨ã€åè§æˆ–æ­§è§†çš„å†…å®¹ã€‚
3. ä¾®è¾±ã€è´¬ä½ç‰¹å®šç¾¤ä½“çš„è¨€è®ºã€‚

#æ­£å¸¸#
æ­£å¸¸èŒƒå›´ï¼š
1. ç”Ÿæ´»ã€å­¦ä¹ ã€å·¥ä½œã€æƒ…æ„Ÿã€æ–°é—»ã€å¨±ä¹ç­‰æ™®é€šå†…å®¹ã€‚
2. ä¸æ¶‰åŠä¸Šè¿°ä»»ä½•è¿è§„å†…å®¹çš„æ­£å½“è¡¨è¾¾ã€‚
3. å®¢è§‚ã€ä¸­æ€§çš„ä¿¡æ¯åˆ†äº«å’Œè®¨è®ºã€‚

##åˆ¤æ–­è¿‡ç¨‹##
ä¸¥æ ¼æŒ‰ç…§ä¸Šè¿°è§„åˆ™ï¼Œä¸€ä¸ªæ ‡ç­¾ä¸€ä¸ªæ ‡ç­¾åˆ¤æ–­ã€‚


##è¾“å‡ºè¦æ±‚##
åªèƒ½è¾“å‡º'æ­£å¸¸'æˆ–'æ”¿æ²»å®‰å…¨'æˆ–'è‰²æƒ…ä½ä¿—'æˆ–'è¿æ³•è¿è§„'æˆ–'æš´æ'æˆ–'æ­§è§†'è¿™6ä¸ªæ ‡ç­¾ä¸­çš„ä¸€ä¸ªï¼Œä¸è¦è¾“å‡ºä»»ä½•å…¶ä»–é¢å¤–çš„ä¿¡æ¯æˆ–è§£é‡Šã€‚

ã€{text}ã€‘

è¯·åˆ¤æ–­ä¸Šè¿°æ–‡æœ¬çš„æ ‡ç­¾ï¼š"""
        prompts.append(prompt)
    
    try:
        # æŠ‘åˆ¶vLLMçš„å†…éƒ¨è¿›åº¦æ¡è¾“å‡º
        with SuppressOutput():
            outputs = llm.generate(prompts, sampling_params)
        
        results = []
        valid_labels = ['æ­£å¸¸', 'æ”¿æ²»å®‰å…¨', 'è‰²æƒ…ä½ä¿—', 'è¿æ³•è¿è§„', 'æš´æ', 'æ­§è§†']
        
        for output in outputs:
            raw_output = output.outputs[0].text.strip()
            
            # æå–æ ‡ç­¾
            found_label = "æ­£å¸¸"  # é»˜è®¤æ ‡ç­¾
            for label in valid_labels:
                if label in raw_output:
                    found_label = label
                    break
            
            results.append(found_label)
        
        return results
        
    except Exception as e:
        print(f"vLLMæ‰¹é‡è°ƒç”¨é”™è¯¯: {e}")
        return ["æ­£å¸¸"] * len(texts)  # é»˜è®¤è¿”å›æ­£å¸¸

def main():
    # è¯»å–æ•°æ®æ–‡ä»¶
    data_file = 'cleaned_data.jsonl'  # ä½¿ç”¨cleaned_data.jsonlæ–‡ä»¶
    print("è¯»å–æ•°æ®æ–‡ä»¶...")
    
    # ç¤ºä¾‹æ•°æ®ï¼ˆå¦‚æœæ²¡æœ‰æ–‡ä»¶ï¼Œä½¿ç”¨ç¤ºä¾‹æ•°æ®è¿›è¡Œæµ‹è¯•ï¼‰
    sample_data = [
        {"text_cn": "æˆ‘åªæƒ³æ‘†è„±è¿™äº›ç—›è‹¦ã€‚æ‹œæ‰˜ï¼Œæˆ‘éœ€è¦ä¸€äº›å¸®åŠ©ï¼Œéœ€è¦æœ‰äººå€¾è¯‰ã€‚", "n_label": "æ­£å¸¸"},
        # å¯ä»¥æ·»åŠ æ›´å¤šç¤ºä¾‹æ•°æ®
    ]
    
    try:
        data = read_json_data(data_file)
        if not data:
            print(f"æ— æ³•è¯»å–æ–‡ä»¶ {data_file}ï¼Œä½¿ç”¨ç¤ºä¾‹æ•°æ®è¿›è¡Œæµ‹è¯•")
            data = sample_data
    except FileNotFoundError:
        print(f"æ–‡ä»¶ {data_file} ä¸å­˜åœ¨ï¼Œä½¿ç”¨ç¤ºä¾‹æ•°æ®è¿›è¡Œæµ‹è¯•")
        data = sample_data
    
    # å­˜å‚¨ç»“æœ
    enhanced_data = []
    correct = 0  # äºŒåˆ†ç±»ä¸€è‡´æ€§
    six_class_correct = 0  # å…­åˆ†ç±»å‡†ç¡®æ€§
    total = len(data)
    true_labels = []
    pred_labels = []
    six_class_true_labels = []  # å…­åˆ†ç±»çœŸå®æ ‡ç­¾
    six_class_pred_labels = []  # å…­åˆ†ç±»é¢„æµ‹æ ‡ç­¾
    detailed_results = []  # å­˜å‚¨è¯¦ç»†ç»“æœç”¨äºExcelè¾“å‡º
    
    print(f"å¼€å§‹å¤„ç† {total} æ¡æ•°æ®...")
    
    # æ‰¹é‡å¤„ç†ä»¥æé«˜æ•ˆç‡ - å¤šå¡å¯ä»¥ä½¿ç”¨æ›´å¤§çš„æ‰¹æ¬¡
    batch_size = 64  # æ ¹æ®æ•°æ®é‡å’ŒGPUæ•°é‡è°ƒæ•´
    print(f"ä½¿ç”¨æ‰¹å¤„ç†å¤§å°: {batch_size}")
    
    # åˆ›å»ºè¾“å‡ºæ–‡ä»¶åå¹¶æ‰“å¼€æ–‡ä»¶ç”¨äºå¢é‡å†™å…¥
    output_filename = f"enhanced_data_{datetime.now().strftime('%Y%m%d_%H%M%S')}.jsonl"
    output_file = open(output_filename, 'w', encoding='utf-8')
    print(f"åˆ›å»ºè¾“å‡ºæ–‡ä»¶: {output_filename}")
    
    # åˆ›å»ºè¿›åº¦æ¡ï¼Œæ˜¾ç¤ºæ›´è¯¦ç»†çš„ä¿¡æ¯
    progress_bar = tqdm(range(0, total, batch_size),
                       desc="å¤„ç†è¿›åº¦",
                       unit="batch",
                       ncols=100,
                       bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [å·²å¤„ç†: {postfix}]')
    
    processed_count = 0
    
    for i in progress_bar:
        batch_data = data[i:i+batch_size]
        texts = [item.get('text_cn', '') for item in batch_data]
        
        # æ‰¹é‡åˆ†ç±»
        predicted_labels = classify_batch(texts)
        
        batch_correct = 0
        batch_six_class_correct = 0
        for j, (item, predicted_label) in enumerate(zip(batch_data, predicted_labels)):
            text = item.get('text_cn', '')
            true_label = item.get('n_label', 'æ­£å¸¸')
            
            # å…­åˆ†ç±»å‡†ç¡®ç‡åˆ¤æ–­
            if true_label == predicted_label:
                six_class_correct += 1
                batch_six_class_correct += 1
            
            # äºŒåˆ†ç±»åˆ¤æ–­é€»è¾‘
            # å°†çœŸå®æ ‡ç­¾è½¬æ¢ä¸ºäºŒåˆ†ç±»ï¼šæ­£å¸¸(1) vs è¿è§„(0)
            true_binary = 1 if true_label == 'æ­£å¸¸' else 0
            
            # å°†é¢„æµ‹æ ‡ç­¾è½¬æ¢ä¸ºäºŒåˆ†ç±»ï¼šæ­£å¸¸(1) vs è¿è§„(0)
            pred_binary = 1 if predicted_label == 'æ­£å¸¸' else 0
            
            # åˆ¤æ–­æ˜¯å¦ä¸€è‡´ï¼ˆ1=ä¸€è‡´, 0=ä¸ä¸€è‡´ï¼‰
            is_consistent = 1 if true_binary == pred_binary else 0
            
            if is_consistent == 1:
                correct += 1
                batch_correct += 1
            
            # åœ¨åŸæœ‰JSONæ•°æ®åŸºç¡€ä¸Šå¢åŠ æ–°å­—æ®µï¼ˆåªå¢åŠ äºŒå…ƒä¸€è‡´æ€§ï¼‰
            enhanced_item = item.copy()  # å¤åˆ¶åŸå§‹æ•°æ®
            enhanced_item['binary_consistency'] = is_consistent  # äºŒå…ƒä¸€è‡´æ€§ï¼š1=ä¸€è‡´, 0=ä¸ä¸€è‡´
            
            enhanced_data.append(enhanced_item)
            true_labels.append(true_binary)
            pred_labels.append(pred_binary)
            six_class_true_labels.append(true_label)
            six_class_pred_labels.append(predicted_label)
            
            # å­˜å‚¨è¯¦ç»†ç»“æœç”¨äºExcelè¾“å‡º
            detailed_results.append({
                'text': text,
                'true_label': true_label,
                'predicted_label': predicted_label,
                'six_class_correct': 1 if true_label == predicted_label else 0,
                'binary_consistency': is_consistent
            })
            
            # ç«‹å³å†™å…¥JSONLæ–‡ä»¶ï¼ˆå¢é‡å†™å…¥ï¼‰
            output_file.write(json.dumps(enhanced_item, ensure_ascii=False) + '\n')
            
            processed_count += 1
        
        # æ¯ä¸ªbatchå¤„ç†å®Œååˆ·æ–°æ–‡ä»¶ç¼“å†²åŒº
        output_file.flush()
        
        # æ›´æ–°è¿›åº¦æ¡æ˜¾ç¤ºä¿¡æ¯
        current_binary_accuracy = correct / processed_count if processed_count > 0 else 0
        current_six_class_accuracy = six_class_correct / processed_count if processed_count > 0 else 0
        progress_bar.set_postfix({
            'æ ·æœ¬': f'{processed_count}/{total}',
            'äºŒåˆ†ç±»': f'{current_binary_accuracy:.1%}',
            'å…­åˆ†ç±»': f'{current_six_class_accuracy:.1%}',
            'æ‰¹æ¬¡': f'{batch_correct}/{len(batch_data)}'
        })
        
        # æ¯å¤„ç†ä¸€å®šæ•°é‡æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯ï¼ˆå¯é€‰ï¼‰
        if processed_count % (batch_size * 100) == 0 or processed_count == total:
            print(f"\nğŸ“Š é˜¶æ®µæ€§ç»Ÿè®¡ [å·²å¤„ç†: {processed_count}/{total}]")
            print(f"   äºŒåˆ†ç±»ä¸€è‡´æ€§å‡†ç¡®ç‡: {current_binary_accuracy:.2%}")
            print(f"   å…­åˆ†ç±»å‡†ç¡®ç‡: {current_six_class_accuracy:.2%}")
            print(f"   äºŒåˆ†ç±»ä¸€è‡´æ ·æœ¬æ•°: {correct}")
            print(f"   å…­åˆ†ç±»æ­£ç¡®æ ·æœ¬æ•°: {six_class_correct}")
            print(f"   âœ… å·²å¢é‡å†™å…¥åˆ°: {output_filename}")
    
    progress_bar.close()
    
    # å…³é—­è¾“å‡ºæ–‡ä»¶
    output_file.close()
    print(f"âœ… JSONLæ–‡ä»¶å†™å…¥å®Œæˆ: {output_filename}")
    
    # è®¡ç®—å‡†ç¡®ç‡
    binary_accuracy = correct / total
    six_class_accuracy = six_class_correct / total
    
    print(f"\n=== è¯„ä¼°ç»“æœæ€»ç»“ ===")
    print(f"æ€»æ ·æœ¬æ•°: {total}")
    print(f"")
    print(f"ğŸ“Š äºŒåˆ†ç±»ä¸€è‡´æ€§ç»“æœ:")
    print(f"   ä¸€è‡´æ ·æœ¬æ•°: {correct}")
    print(f"   ä¸ä¸€è‡´æ ·æœ¬æ•°: {total - correct}")
    print(f"   ä¸€è‡´æ€§å‡†ç¡®ç‡: {binary_accuracy:.2%}")
    print(f"")
    print(f"ğŸ“Š å…­åˆ†ç±»å‡†ç¡®ç‡ç»“æœ:")
    print(f"   æ­£ç¡®æ ·æœ¬æ•°: {six_class_correct}")
    print(f"   é”™è¯¯æ ·æœ¬æ•°: {total - six_class_correct}")
    print(f"   å…­åˆ†ç±»å‡†ç¡®ç‡: {six_class_accuracy:.2%}")
    
    # ç”ŸæˆäºŒåˆ†ç±»æŠ¥å‘Š
    if len(set(true_labels)) > 1 and len(set(pred_labels)) > 1:
        report = classification_report(true_labels, pred_labels, output_dict=True, target_names=['è¿è§„', 'æ­£å¸¸'])
        print("\n=== äºŒåˆ†ç±»è¯¦ç»†æŠ¥å‘Š ===")
        for label, metrics in report.items():
            if label in ['0', '1']:  # 0=è¿è§„, 1=æ­£å¸¸
                label_name = 'è¿è§„' if label == '0' else 'æ­£å¸¸'
                print(f"{label_name}: ç²¾ç¡®ç‡={metrics['precision']:.2%}, å¬å›ç‡={metrics['recall']:.2%}, F1åˆ†æ•°={metrics['f1-score']:.2%}")
    
    # ç”Ÿæˆå…­åˆ†ç±»æŠ¥å‘Š
    if len(set(six_class_true_labels)) > 1 and len(set(six_class_pred_labels)) > 1:
        six_class_labels = ['æ­£å¸¸', 'æ”¿æ²»å®‰å…¨', 'è‰²æƒ…ä½ä¿—', 'è¿æ³•è¿è§„', 'æš´æ', 'æ­§è§†']
        available_labels = list(set(six_class_true_labels + six_class_pred_labels))
        six_class_report = classification_report(six_class_true_labels, six_class_pred_labels,
                                               output_dict=True,
                                               target_names=available_labels,
                                               labels=available_labels)
        print("\n=== å…­åˆ†ç±»è¯¦ç»†æŠ¥å‘Š ===")
        for label in available_labels:
            if label in six_class_report:
                metrics = six_class_report[label]
                print(f"{label}: ç²¾ç¡®ç‡={metrics['precision']:.2%}, å¬å›ç‡={metrics['recall']:.2%}, F1åˆ†æ•°={metrics['f1-score']:.2%}")
    
    # è®¡ç®—å„ç§æƒ…å†µçš„ç»Ÿè®¡ï¼ˆä½¿ç”¨detailed_resultsï¼Œå› ä¸ºenhanced_dataä¸­æ²¡æœ‰predicted_labelï¼‰
    normal_to_normal = sum(1 for item in detailed_results
                          if item['true_label'] == 'æ­£å¸¸' and item['predicted_label'] == 'æ­£å¸¸')
    normal_to_violation = sum(1 for item in detailed_results
                             if item['true_label'] == 'æ­£å¸¸' and item['predicted_label'] != 'æ­£å¸¸')
    violation_to_normal = sum(1 for item in detailed_results
                             if item['true_label'] != 'æ­£å¸¸' and item['predicted_label'] == 'æ­£å¸¸')
    violation_to_violation = sum(1 for item in detailed_results
                                if item['true_label'] != 'æ­£å¸¸' and item['predicted_label'] != 'æ­£å¸¸')
    
    print(f"\n=== è¯¦ç»†ç»Ÿè®¡ ===")
    print(f"æ­£å¸¸â†’æ­£å¸¸: {normal_to_normal}")
    print(f"æ­£å¸¸â†’è¿è§„: {normal_to_violation}")
    print(f"è¿è§„â†’æ­£å¸¸: {violation_to_normal}")
    print(f"è¿è§„â†’è¿è§„: {violation_to_violation}")
    
    # åˆ›å»ºExcelæ–‡ä»¶ï¼ŒåŒ…å«è¯¦ç»†ä¿¡æ¯
    excel_filename = f"detailed_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx"
    df = pd.DataFrame(detailed_results)
    df.to_excel(excel_filename, index=False, sheet_name='è¯¦ç»†ç»“æœ')
    
    print(f"\nğŸ“Š Excelè¯¦ç»†ä¿¡æ¯å·²ä¿å­˜åˆ°: {excel_filename}")
    print(f"   åŒ…å«å­—æ®µ: æ–‡æœ¬ã€çœŸå®æ ‡ç­¾ã€é¢„æµ‹æ ‡ç­¾ã€å…­åˆ†ç±»æ­£ç¡®æ€§ã€äºŒå…ƒä¸€è‡´æ€§")
    
    # ä¿å­˜ç»Ÿè®¡æ‘˜è¦
    summary = {
        'total_cases': total,
        'binary_consistency_accuracy': binary_accuracy,
        'six_class_accuracy': six_class_accuracy,
        'binary_correct': correct,
        'six_class_correct': six_class_correct,
        'normal_to_normal': normal_to_normal,
        'normal_to_violation': normal_to_violation,
        'violation_to_normal': violation_to_normal,
        'violation_to_violation': violation_to_violation,
        'timestamp': datetime.now().strftime('%Y%m%d_%H%M%S')
    }
    
    summary_filename = f"summary_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    with open(summary_filename, 'w', encoding='utf-8') as f:
        json.dump(summary, f, ensure_ascii=False, indent=2)
    
    print(f"\n=== æ–‡ä»¶ä¿å­˜æ€»ç»“ ===")
    print(f"ğŸ“„ å¢å¼ºæ•°æ®JSONL: {output_filename}")
    print(f"   - åŸå§‹æ•°æ® + binary_consistencyå­—æ®µ")
    print(f"ğŸ“Š è¯¦ç»†ç»“æœExcel: {excel_filename}")
    print(f"   - æ¯æ¡æ•°æ®çš„æ–‡æœ¬ã€æ ‡ç­¾ã€é¢„æµ‹ç»“æœ")
    print(f"ğŸ“‹ ç»Ÿè®¡æ‘˜è¦JSON: {summary_filename}")
    print(f"   - äºŒåˆ†ç±»å’Œå…­åˆ†ç±»å‡†ç¡®ç‡ç»Ÿè®¡")
    print(f"")
    print(f"ğŸ¯ æœ€ç»ˆç»“æœ:")
    print(f"   äºŒåˆ†ç±»ä¸€è‡´æ€§å‡†ç¡®ç‡: {binary_accuracy:.2%}")
    print(f"   å…­åˆ†ç±»å‡†ç¡®ç‡: {six_class_accuracy:.2%}")

if __name__ == "__main__":
    main()